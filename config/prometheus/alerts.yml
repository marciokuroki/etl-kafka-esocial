# config/prometheus/alerts.yml
# Regras de Alerta - Pipeline ETL eSocial com Apache Kafka
# Vers√£o: 2.1 - Completa e Corrigida
# √öltima atualiza√ß√£o: 2025-11-23


groups:
  # ========================================
  # GRUPO 1: ALERTAS CR√çTICOS DO PRODUCER
  # ========================================
  - name: producer_critical_alerts
    interval: 30s
    rules:
      
      # Servi√ßo Producer Indispon√≠vel
      - alert: ProducerServiceDown
        expr: |
          up{job="producer-service"} == 0
        for: 1m
        labels:
          severity: critical
          component: producer
          category: availability
          team: platform
        annotations:
          summary: "üö® Producer Service indispon√≠vel"
          description: |
            O Producer Service n√£o est√° respondendo h√° mais de 1 minuto.
            Status: DOWN
            Job: {{ $labels.job }}
            Instance: {{ $labels.instance }}
          action: |
            1. Verificar status do container:
               docker ps -a | grep producer-service
            
            2. Ver logs recentes:
               docker logs esocial-producer --tail 100
            
            3. Verificar conectividade com depend√™ncias:
               - PostgreSQL: docker exec esocial-postgres-db pg_isready
               - Kafka: curl http://localhost:8090/api/clusters
            
            4. Reiniciar servi√ßo:
               docker-compose restart producer-service
          runbook_url: "https://github.com/marciokuroki/etl-kafka-esocial/wiki/Runbook-ProducerServiceDown"


      # Alta Taxa de Erro no Producer
      - alert: ProducerHighErrorRate
        expr: |
          rate(events_failed_total{service="producer"}[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
          component: producer
          category: data-quality
          team: data-engineering
        annotations:
          summary: "‚ö†Ô∏è Producer com alta taxa de erro"
          description: |
            Taxa de erro no Producer est√° em {{ $value | humanizePercentage }}.
            Limite: 5%
            Servi√ßo: Producer Service
          action: |
            1. Consultar m√©tricas detalhadas:
               curl http://localhost:8081/actuator/prometheus | grep events_failed
            
            2. Verificar logs de erro:
               docker logs esocial-producer --tail 50 | grep ERROR
            
            3. Verificar conectividade com banco de origem:
               docker exec esocial-postgres-db psql -U esocial_user -d esocial -c "SELECT 1;"
            
            4. Analisar √∫ltimas mudan√ßas no schema da origem
          runbook_url: "https://github.com/marciokuroki/etl-kafka-esocial/wiki/Runbook-ProducerHighErrorRate"


      # CDC com Lat√™ncia Elevada
      - alert: CDCHighLatency
        expr: |
          histogram_quantile(0.95, rate(cdc_polling_duration_seconds_bucket{service="producer"}[5m])) > 10
        for: 5m
        labels:
          severity: warning
          component: producer-cdc
          category: performance
          team: data-engineering
        annotations:
          summary: "‚è±Ô∏è CDC com lat√™ncia elevada"
          description: |
            Lat√™ncia P95 do CDC est√° em {{ $value }}s.
            Limite: 10 segundos
            Poss√≠vel gargalo no polling ou no banco de dados.
          action: |
            1. Verificar carga do PostgreSQL:
               docker exec esocial-postgres-db psql -U esocial_user -d esocial -c "SELECT * FROM pg_stat_activity WHERE state = 'active';"
            
            2. Analisar volume de mudan√ßas:
               docker exec esocial-postgres-db psql -U esocial_user -d esocial -c "SELECT COUNT(*) FROM source.employees WHERE updated_at > NOW() - INTERVAL '5 minutes';"
            
            3. Verificar configura√ß√£o de polling interval
            
            4. Considerar otimizar queries do CDC
          runbook_url: "https://github.com/marciokuroki/etl-kafka-esocial/wiki/Runbook-CDCHighLatency"


      # Throughput Baixo no Producer
      - alert: ProducerLowThroughput
        expr: |
          rate(events_published_total{service="producer"}[5m]) * 60 < 1
        for: 10m
        labels:
          severity: warning
          component: producer
          category: performance
          team: data-engineering
        annotations:
          summary: "üìâ Throughput baixo no Producer"
          description: |
            Producer publicando apenas {{ $value }} eventos/min.
            Esperado: > 1 evento/min
          action: |
            1. Verificar se h√° dados novos na origem
            2. Analisar lat√™ncia do CDC
            3. Verificar conectividade com Kafka
          runbook_url: "https://github.com/marciokuroki/etl-kafka-esocial/wiki/Runbook-LowThroughput"


  # ========================================
  # GRUPO 2: ALERTAS CR√çTICOS DO CONSUMER
  # ========================================
  - name: consumer_critical_alerts
    interval: 30s
    rules:
      
      # Servi√ßo Consumer Indispon√≠vel
      - alert: ConsumerServiceDown
        expr: |
          up{job="consumer-service"} == 0
        for: 1m
        labels:
          severity: critical
          component: consumer
          category: availability
          team: platform
        annotations:
          summary: "üö® Consumer Service indispon√≠vel"
          description: |
            O Consumer Service n√£o est√° respondendo.
            Status: DOWN
            Job: {{ $labels.job }}
          action: |
            1. Verificar container:
               docker ps -a | grep consumer-service
            
            2. Ver logs:
               docker logs esocial-consumer --tail 100
            
            3. Verificar depend√™ncias:
               - Kafka: curl http://localhost:8090
               - PostgreSQL: docker exec esocial-postgres-db pg_isready
            
            4. Reiniciar:
               docker-compose restart consumer-service
          runbook_url: "https://github.com/marciokuroki/etl-kafka-esocial/wiki/Runbook-ConsumerServiceDown"


      # Alta Taxa de Erro de Valida√ß√£o
      - alert: ConsumerHighValidationErrorRate
        expr: |
          (
            rate(validation_failure_total{service="consumer",severity="ERROR"}[5m]) 
            / 
            (rate(validation_success_total{service="consumer"}[5m]) + rate(validation_failure_total{service="consumer",severity="ERROR"}[5m]))
          ) > 0.05
        for: 5m
        labels:
          severity: critical
          component: consumer
          category: data-quality
          team: data-engineering
        annotations:
          summary: "‚ö†Ô∏è Alta taxa de erro de valida√ß√£o"
          description: |
            Taxa de falha nas valida√ß√µes: {{ $value | humanizePercentage }}.
            Limite: 5%
            Servi√ßo: Consumer Service
          action: |
            1. Consultar erros via API:
               curl http://localhost:8082/api/v1/validation/errors | jq
            
            2. Ver distribui√ß√£o de erros:
               curl http://localhost:8082/api/v1/validation/dashboard | jq
            
            3. Verificar logs:
               docker logs esocial-consumer --tail 100 | grep "Validation failed"
            
            4. Analisar padr√£o de erros para identificar causa raiz
          runbook_url: "https://github.com/marciokuroki/etl-kafka-esocial/wiki/Runbook-HighValidationErrorRate"


      # DLQ Acumulando (Warning)
      - alert: DLQAccumulating
        expr: |
          dlq_events_pending{service="consumer"} > 100
        for: 10m
        labels:
          severity: warning
          component: consumer-dlq
          category: data-processing
          team: data-engineering
        annotations:
          summary: "üì¨ DLQ com eventos acumulados"
          description: |
            {{ $value }} eventos acumulados na Dead Letter Queue.
            Limite Warning: 100 eventos
            Requer aten√ß√£o para evitar ac√∫mulo cr√≠tico.
          action: |
            1. Consultar eventos DLQ:
               curl http://localhost:8082/api/v1/validation/dlq | jq
            
            2. Analisar padr√µes de erro:
               curl http://localhost:8082/api/v1/validation/dlq | jq '.[] | .errorMessage' | sort | uniq -c
            
            3. Verificar se causa raiz j√° foi corrigida
            
            4. Considerar reprocessamento em lote
          runbook_url: "https://github.com/marciokuroki/etl-kafka-esocial/wiki/Runbook-DLQAccumulating"


      # DLQ Cr√≠tica (CONTINUA√á√ÉO COMPLETA)
      - alert: DLQCritical
        expr: |
          dlq_events_pending{service="consumer"} > 500
        for: 5m
        labels:
          severity: critical
          component: consumer-dlq
          category: data-processing
          team: data-engineering
        annotations:
          summary: "üö® DLQ CR√çTICA - Muitos eventos pendentes"
          description: |
            {{ $value }} eventos na DLQ - situa√ß√£o cr√≠tica!
            Limite Critical: 500 eventos
            Requer interven√ß√£o IMEDIATA.
          action: |
            1. URGENTE: Consultar eventos DLQ:
               curl http://localhost:8082/api/v1/validation/dlq | jq
            
            2. Identificar causa raiz dos erros:
               curl http://localhost:8082/api/v1/validation/dlq | jq 'group_by(.errorType) | map({type: .[0].errorType, count: length}) | sort_by(.count) | reverse'
            
            3. Se causa raiz corrigida, reprocessar em lote:
               for id in $(curl -s http://localhost:8082/api/v1/validation/dlq | jq -r '.[].id'); do
                 curl -X POST http://localhost:8082/api/v1/validation/dlq/$id/retry
                 sleep 0.5
               done
            
            4. Considerar pausar Producer temporariamente:
               docker-compose stop producer-service
            
            5. Ap√≥s corre√ß√£o, reativar Producer:
               docker-compose start producer-service
            
            6. Comunicar time de neg√≥cio sobre atraso no processamento
          runbook_url: "https://github.com/marciokuroki/etl-kafka-esocial/wiki/Runbook-DLQCritical"


      # Lat√™ncia de Valida√ß√£o Alta
      - alert: ConsumerHighValidationLatency
        expr: |
          histogram_quantile(0.95, rate(validation_duration_seconds_bucket{service="consumer"}[5m])) > 0.5
        for: 5m
        labels:
          severity: warning
          component: consumer
          category: performance
          team: data-engineering
        annotations:
          summary: "‚è±Ô∏è Lat√™ncia alta nas valida√ß√µes"
          description: |
            Lat√™ncia P95 das valida√ß√µes: {{ $value }}s.
            Limite: 500ms
            Pode indicar gargalo no processamento.
          action: |
            1. Verificar carga do Consumer:
               docker stats esocial-consumer --no-stream
            
            2. Analisar queries lentas no PostgreSQL:
               docker exec esocial-postgres-db psql -U esocial_user -d esocial -c "SELECT query, mean_exec_time FROM pg_stat_statements ORDER BY mean_exec_time DESC LIMIT 10;"
            
            3. Verificar se h√° valida√ß√µes complexas sendo executadas
            
            4. Considerar otimizar valida√ß√µes ou adicionar √≠ndices no banco
          runbook_url: "https://github.com/marciokuroki/etl-kafka-esocial/wiki/Runbook-HighValidationLatency"


  # ========================================
  # GRUPO 3: ALERTAS DO KAFKA
  # ========================================
  - name: kafka_alerts
    interval: 30s
    rules:
      
      # Broker Kafka Down
      - alert: KafkaBrokerDown
        expr: |
          up{job="kafka-broker"} == 0
        for: 2m
        labels:
          severity: critical
          component: kafka
          category: availability
          team: platform
        annotations:
          summary: "üö® Kafka Broker indispon√≠vel"
          description: |
            Broker Kafka n√£o est√° respondendo.
            Instance: {{ $labels.instance }}
            Poss√≠vel perda de replica√ß√£o se m√∫ltiplos brokers estiverem down.
          action: |
            1. Verificar status dos brokers:
               docker ps -a | grep kafka-broker
            
            2. Ver logs do broker:
               docker logs esocial-kafka-broker-1 --tail 100
            
            3. Verificar Zookeeper:
               docker exec esocial-zookeeper zkCli.sh ls /brokers/ids
            
            4. Reiniciar broker:
               docker-compose restart kafka-broker-1
            
            5. Validar ISR (In-Sync Replicas):
               docker exec esocial-kafka-broker-1 kafka-topics --bootstrap-server localhost:9092 --describe
          runbook_url: "https://github.com/marciokuroki/etl-kafka-esocial/wiki/Runbook-KafkaBrokerDown"


      # Consumer Lag Alto
      - alert: KafkaConsumerLagHigh
        expr: |
          kafka_consumergroup_lag{group="esocial-consumer-group"} > 1000
        for: 5m
        labels:
          severity: warning
          component: kafka
          category: performance
          team: data-engineering
        annotations:
          summary: "üìä Consumer lag alto"
          description: |
            Lag do consumer group est√° em {{ $value }} mensagens.
            Limite: 1000 mensagens
            Consumer n√£o est√° acompanhando a produ√ß√£o.
          action: |
            1. Verificar lag detalhado:
               docker exec esocial-kafka-broker-1 kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group esocial-consumer-group
            
            2. Verificar se Consumer est√° saud√°vel:
               curl http://localhost:8082/actuator/health | jq
            
            3. Analisar throughput do Consumer:
               curl http://localhost:8082/actuator/prometheus | grep events_consumed_total
            
            4. Considerar escalar Consumer (adicionar inst√¢ncias)
          runbook_url: "https://github.com/marciokuroki/etl-kafka-esocial/wiki/Runbook-HighConsumerLag"


      # Consumer Lag Cr√≠tico
      - alert: KafkaConsumerLagCritical
        expr: |
          kafka_consumergroup_lag{group="esocial-consumer-group"} > 5000
        for: 5m
        labels:
          severity: critical
          component: kafka
          category: performance
          team: data-engineering
        annotations:
          summary: "üö® Consumer lag CR√çTICO"
          description: |
            Lag CR√çTICO: {{ $value }} mensagens!
            Sistema pode estar subdimensionado.
          action: |
            1. URGENTE: Verificar Consumer:
               docker logs esocial-consumer --tail 200 | grep ERROR
            
            2. Escalar Consumer imediatamente:
               docker-compose up -d --scale consumer-service=3
            
            3. Considerar pausar Producer temporariamente
            
            4. An√°lise p√≥s-incidente necess√°ria
          runbook_url: "https://github.com/marciokuroki/etl-kafka-esocial/wiki/Runbook-CriticalConsumerLag"


      # Under Replicated Partitions
      - alert: KafkaUnderReplicatedPartitions
        expr: |
          kafka_cluster_partition_underreplicated > 0
        for: 5m
        labels:
          severity: critical
          component: kafka
          category: data-integrity
          team: platform
        annotations:
          summary: "‚ö†Ô∏è Parti√ß√µes sem replica√ß√£o completa"
          description: |
            {{ $value }} parti√ß√µes est√£o sub-replicadas.
            Risco de perda de dados se broker com l√≠der cair.
          action: |
            1. Verificar ISR (In-Sync Replicas):
               docker exec esocial-kafka-broker-1 kafka-topics --bootstrap-server localhost:9092 --describe --under-replicated-partitions
            
            2. Verificar status dos brokers:
               docker ps | grep kafka-broker
            
            3. Ver logs dos brokers:
               docker logs esocial-kafka-broker-1 --tail 100
               docker logs esocial-kafka-broker-2 --tail 100
               docker logs esocial-kafka-broker-3 --tail 100
            
            4. Se broker down, reiniciar:
               docker-compose restart kafka-broker-X
          runbook_url: "https://github.com/marciokuroki/etl-kafka-esocial/wiki/Runbook-UnderReplicatedPartitions"


  # ========================================
  # GRUPO 4: ALERTAS DO POSTGRESQL
  # ========================================
  - name: postgresql_alerts
    interval: 30s
    rules:
      
      # PostgreSQL Down
      - alert: PostgreSQLDown
        expr: |
          up{job="postgresql"} == 0
        for: 1m
        labels:
          severity: critical
          component: postgresql
          category: availability
          team: platform
        annotations:
          summary: "üö® PostgreSQL indispon√≠vel"
          description: |
            Banco de dados PostgreSQL n√£o est√° respondendo.
            Instance: {{ $labels.instance }}
          action: |
            1. Verificar container:
               docker ps -a | grep postgres
            
            2. Ver logs:
               docker logs esocial-postgres-db --tail 100
            
            3. Verificar se o processo est√° rodando:
               docker exec esocial-postgres-db pg_isready -U esocial_user
            
            4. Reiniciar PostgreSQL:
               docker-compose restart postgres-db
            
            5. Validar conectividade:
               docker exec esocial-postgres-db psql -U esocial_user -d esocial -c "SELECT 1;"
          runbook_url: "https://github.com/marciokuroki/etl-kafka-esocial/wiki/Runbook-PostgreSQLDown"


      # Conex√µes PostgreSQL Alto
      - alert: PostgreSQLHighConnections
        expr: |
          pg_stat_database_numbackends{datname="esocial"} > 80
        for: 5m
        labels:
          severity: warning
          component: postgresql
          category: performance
          team: platform
        annotations:
          summary: "‚ö†Ô∏è N√∫mero alto de conex√µes no PostgreSQL"
          description: |
            {{ $value }} conex√µes ativas no banco esocial.
            Limite: 80 (de 100 max)
          action: |
            1. Ver conex√µes ativas:
               docker exec esocial-postgres-db psql -U esocial_user -d esocial -c "SELECT count(*) FROM pg_stat_activity WHERE state = 'active';"
            
            2. Identificar queries lentas:
               docker exec esocial-postgres-db psql -U esocial_user -d esocial -c "SELECT pid, state, query_start, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY query_start;"
            
            3. Verificar configura√ß√£o de connection pooling (HikariCP)
            
            4. Considerar matar queries longas (com cautela):
               SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE state = 'active' AND query_start < NOW() - INTERVAL '10 minutes';
          runbook_url: "https://github.com/marciokuroki/etl-kafka-esocial/wiki/Runbook-HighPostgreSQLConnections"


      # Espa√ßo em Disco PostgreSQL Baixo
      - alert: PostgreSQLDiskSpaceLow
        expr: |
          (node_filesystem_avail_bytes{mountpoint="/var/lib/postgresql/data"} / node_filesystem_size_bytes{mountpoint="/var/lib/postgresql/data"}) * 100 < 20
        for: 5m
        labels:
          severity: warning
          component: postgresql
          category: storage
          team: platform
        annotations:
          summary: "üíæ Espa√ßo em disco baixo no PostgreSQL"
          description: |
            Apenas {{ $value }}% de espa√ßo dispon√≠vel.
            Limite: 20%
          action: |
            1. Verificar uso de disco:
               docker exec esocial-postgres-db df -h /var/lib/postgresql/data
            
            2. Ver tamanho dos bancos:
               docker exec esocial-postgres-db psql -U esocial_user -d esocial -c "SELECT pg_database.datname, pg_size_pretty(pg_database_size(pg_database.datname)) FROM pg_database;"
            
            3. Executar VACUUM:
               docker exec esocial-postgres-db psql -U esocial_user -d esocial -c "VACUUM FULL ANALYZE;"
            
            4. Limpar logs antigos:
               docker exec esocial-postgres-db find /var/lib/postgresql/data/log -name "*.log" -mtime +7 -delete
            
            5. Se necess√°rio, aumentar volume Docker
          runbook_url: "https://github.com/marciokuroki/etl-kafka-esocial/wiki/Runbook-LowDiskSpace"


  # ========================================
  # GRUPO 5: ALERTAS DE PERFORMANCE
  # ========================================
  - name: performance_alerts
    interval: 30s
    rules:
      
      # CPU Alta
      - alert: HighCPUUsage
        expr: |
          (100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 80
        for: 10m
        labels:
          severity: warning
          component: infrastructure
          category: performance
          team: platform
        annotations:
          summary: "üî• CPU alta detectada"
          description: |
            CPU em {{ $value }}% no host {{ $labels.instance }}.
            Limite: 80%
          action: |
            1. Verificar processos:
               docker stats --no-stream
            
            2. Identificar container consumindo CPU:
               docker ps -q | xargs docker stats --no-stream --format "table {{.Container}}\t{{.CPUPerc}}" | sort -k 2 -r
            
            3. Analisar logs dos servi√ßos de maior consumo
            
            4. Considerar escalar recursos ou otimizar c√≥digo
          runbook_url: "https://github.com/marciokuroki/etl-kafka-esocial/wiki/Runbook-HighCPU"


      # Mem√≥ria Alta
      - alert: HighMemoryUsage
        expr: |
          (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 10m
        labels:
          severity: warning
          component: infrastructure
          category: performance
          team: platform
        annotations:
          summary: "üß† Mem√≥ria alta detectada"
          description: |
            Mem√≥ria em {{ $value }}% no host {{ $labels.instance }}.
            Limite: 85%
          action: |
            1. Verificar uso de mem√≥ria por container:
               docker stats --no-stream --format "table {{.Container}}\t{{.MemUsage}}\t{{.MemPerc}}" | sort -k 3 -r
            
            2. Verificar se h√° memory leaks:
               docker logs esocial-producer | grep -i "OutOfMemoryError"
               docker logs esocial-consumer | grep -i "OutOfMemoryError"
            
            3. Analisar heap dumps (se dispon√≠vel)
            
            4. Considerar aumentar mem√≥ria ou otimizar aplica√ß√µes
          runbook_url: "https://github.com/marciokuroki/etl-kafka-esocial/wiki/Runbook-HighMemory"


      # JVM Memory Pressure
      - alert: JVMMemoryPressure
        expr: |
          (jvm_memory_used_bytes{area="heap"} / jvm_memory_max_bytes{area="heap"}) * 100 > 80
        for: 5m
        labels:
          severity: warning
          component: jvm
          category: performance
          team: data-engineering
        annotations:
          summary: "‚òï JVM com press√£o de mem√≥ria"
          description: |
            Heap JVM em {{ $value }}% no servi√ßo {{ $labels.service }}.
            Limite: 80%
          action: |
            1. Verificar m√©tricas JVM:
               curl http://localhost:{{ $labels.port }}/actuator/metrics/jvm.memory.used | jq
            
            2. Ver garbage collection:
               curl http://localhost:{{ $labels.port }}/actuator/metrics/jvm.gc.pause | jq
            
            3. Verificar configura√ß√£o de heap:
               docker inspect {{ $labels.container }} | jq '.[0].Config.Env' | grep JAVA_OPTS
            
            4. Considerar aumentar heap (-Xmx) ou otimizar c√≥digo
          runbook_url: "https://github.com/marciokuroki/etl-kafka-esocial/wiki/Runbook-JVMMemoryPressure"


  # ========================================
  # GRUPO 6: ALERTAS DE RECURSOS
  # ========================================
  - name: resource_alerts
    interval: 1m
    rules:
      
      # Container Restartando Frequentemente
      - alert: ContainerRestartingFrequently
        expr: |
          rate(container_last_seen{name=~"esocial-.*"}[5m]) > 0
        for: 10m
        labels:
          severity: warning
          component: docker
          category: stability
          team: platform
        annotations:
          summary: "üîÑ Container restartando frequentemente"
          description: |
            Container {{ $labels.name }} reiniciou {{ $value }} vezes nos √∫ltimos 5 minutos.
          action: |
            1. Ver hist√≥rico de restarts:
               docker ps -a | grep {{ $labels.name }}
            
            2. Verificar logs:
               docker logs {{ $labels.name }} --tail 200
            
            3. Verificar health checks:
               docker inspect {{ $labels.name }} | jq '.[0].State.Health'
            
            4. Identificar causa raiz (OOM? Crash? Config?)
          runbook_url: "https://github.com/marciokuroki/etl-kafka-esocial/wiki/Runbook-FrequentRestarts"


      # Disco Raiz Quase Cheio
      - alert: DiskSpaceCritical
        expr: |
          (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100 < 10
        for: 5m
        labels:
          severity: critical
          component: infrastructure
          category: storage
          team: platform
        annotations:
          summary: "üíæ DISCO CR√çTICO - Apenas {{ $value }}% dispon√≠vel"
          description: |
            Espa√ßo em disco criticamente baixo.
            Sistema pode parar de funcionar!
          action: |
            1. URGENTE: Verificar uso de disco:
               df -h
            
            2. Limpar logs Docker antigos:
               docker system prune -a --volumes -f
            
            3. Limpar imagens n√£o utilizadas:
               docker image prune -a -f
            
            4. Verificar logs da aplica√ß√£o:
               du -sh /var/lib/docker/containers/*
            
            5. Considerar aumentar volume ou mover dados
          runbook_url: "https://github.com/marciokuroki/etl-kafka-esocial/wiki/Runbook-CriticalDiskSpace"


# ========================================
# FIM DO ARQUIVO DE ALERTAS
# ========================================
# Total de alertas: 23
# Grupos: 6
# Criticidade: 10 critical, 13 warning
# √öltima revis√£o: 2025-11-23 por M√°rcio Kuroki
