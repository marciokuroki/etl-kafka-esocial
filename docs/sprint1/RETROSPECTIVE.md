# Retrospectiva Sprint 1

**Data:** 30/11/2025  
**Participantes:** Equipe de desenvolvimento  
**Facilitador:** MÃ¡rcio Kuroki

## ğŸ¯ Objetivos da Sprint

- [x] Criar infraestrutura base com Kafka
- [x] Implementar Producer Service
- [x] Implementar Consumer Service
- [x] Configurar observabilidade
- [x] Documentar arquitetura

## â­ O que funcionou bem (Keep)

### 1. Docker Compose
**Voto:** â­â­â­â­â­  
ContainerizaÃ§Ã£o de todos os serviÃ§os facilitou enormemente o desenvolvimento e garantiu consistÃªncia entre ambientes.

**EvidÃªncias:**
- Zero problemas de "funciona na minha mÃ¡quina"
- Setup completo em 5 minutos
- 14 containers rodando harmonicamente

**AÃ§Ã£o:** Continuar expandindo docker-compose nas prÃ³ximas sprints.

### 2. MÃ©tricas desde o inÃ­cio
**Voto:** â­â­â­â­  
Prometheus + Grafana desde o comeÃ§o permitiu identificar problemas de performance rapidamente.

**EvidÃªncias:**
- LatÃªncia P95 monitorada desde o primeiro deploy
- 3 problemas de performance identificados precocemente
- Dashboards prontos para demonstraÃ§Ã£o

### 3. Testes UnitÃ¡rios
**Voto:** â­â­â­â­  
18 testes do Producer pegaram 4 bugs antes de ir para integraÃ§Ã£o.

**EvidÃªncias:**
- Cobertura de 82% (acima da meta de 70%)
- 4 bugs encontrados e corrigidos antes de integraÃ§Ã£o
- Refactoring com confianÃ§a

## ğŸ”„ O que pode melhorar (Improve)

### 1. Testes de IntegraÃ§Ã£o ausentes
**Impacto:** MÃ©dio  
**Problema:** Falta de testes end-to-end completos.

**AÃ§Ãµes:**
- [ ] Priorizar testes de integraÃ§Ã£o na Sprint 2
- [ ] Criar suite de testes com Testcontainers
- [ ] Automatizar no CI/CD

### 2. DocumentaÃ§Ã£o tardia
**Impacto:** Baixo  
**Problema:** ADRs criados ao final da sprint.

**AÃ§Ãµes:**
- [ ] Documentar decisÃµes no momento em que sÃ£o tomadas
- [ ] Template de ADR no repositÃ³rio
- [ ] Reminder no DoD dos cards

### 3. Falta de alertas
**Impacto:** MÃ©dio  
**Problema:** MÃ©tricas coletadas mas sem alertas configurados.

**AÃ§Ãµes:**
- [ ] Configurar alertas Prometheus (Sprint 2)
- [ ] Definir SLOs e SLIs
- [ ] Integrar com Slack/email

## ğŸš« O que nÃ£o funcionou (Drop)

### 1. Oracle XE na POC
**DecisÃ£o:** Remover Oracle, manter PostgreSQL  
**Justificativa:** Complexidade nÃ£o justificada para POC

**AÃ§Ã£o:** Manter PostgreSQL como padrÃ£o, avaliar Oracle apenas em produÃ§Ã£o.

### 2. Polling interval muito frequente (2s)
**DecisÃ£o:** Aumentar para 5s  
**Justificativa:** 2s causava carga desnecessÃ¡ria no banco

**EvidÃªncia:** CPU do PostgreSQL caiu de 15% para 8% apÃ³s ajuste.

## ğŸ“Š MÃ©tricas da Sprint

### Velocidade
- Story Points planejados: 21
- Story Points entregues: 21
- Velocidade: 21 SP/sprint

### Qualidade
- Bugs encontrados: 7
- Bugs resolvidos: 7
- Bugs em produÃ§Ã£o: 0
- Cobertura de testes: 82%

### EficiÃªncia
- Lead time mÃ©dio: 2,8 dias
- Cycle time mÃ©dio: 1,5 dias
- Tempo de revisÃ£o: 0,3 dias

## ğŸ¬ Action Items

| AÃ§Ã£o | ResponsÃ¡vel | Prazo | Status |
|------|-------------|-------|--------|
| Criar testes de integraÃ§Ã£o | MÃ¡rcio | Sprint 2 | ğŸ”„ Todo |
| Configurar alertas Prometheus | MÃ¡rcio | Sprint 2 | ğŸ”„ Todo |
| Documentar troubleshooting | MÃ¡rcio | Sprint 2 | ğŸ”„ Todo |
| Criar dashboards Grafana | MÃ¡rcio | Sprint 2 | ğŸ”„ Todo |
| Implementar CI/CD bÃ¡sico | MÃ¡rcio | Sprint 3 | ğŸ“‹ Backlog |

## ğŸ† CelebraÃ§Ãµes

- âœ¨ Zero downtime durante toda a sprint
- âœ¨ 100% dos story points entregues
- âœ¨ Cobertura de testes superou meta (82% vs 70%)
- âœ¨ LatÃªncia P95 melhor que esperado (100ms vs 150ms)
- âœ¨ DocumentaÃ§Ã£o arquitetural completa

## ğŸ’¡ Insights

### TÃ©cnicos
1. Kafka Ã© mais fÃ¡cil de operar do que esperÃ¡vamos
2. Polling CDC Ã© suficiente para volumes moderados
3. ValidaÃ§Ã£o em camadas facilita manutenÃ§Ã£o

### Processo
1. Daily standups de 10min sÃ£o suficientes
2. Code review ajuda muito na qualidade
3. Documentar decisÃµes (ADR) evita retrabalho

### Pessoal
1. Prazos apertados mas realistas
2. Trabalho solo requer disciplina maior
3. Ferramentas certas fazem diferenÃ§a enorme
