# Plano de TransiÃ§Ã£o para ProduÃ§Ã£o - Pipeline ETL eSocial

**VersÃ£o:** 1.0  
**Data:** 2025-11-22  
**Projeto:** Pipeline ETL eSocial  
**ResponsÃ¡vel:** MÃ¡rcio Kuroki GonÃ§alves

**âš ï¸ IMPORTANTE:** Este Ã© um plano **teÃ³rico/simulado** desenvolvido para o Projeto Aplicado. Em ambiente corporativo real, este plano seria validado com as Ã¡reas de Infraestrutura, SeguranÃ§a, Compliance e Business.

---

## ğŸ“‹ Ãndice

1. [VisÃ£o Geral](#visÃ£o-geral)
2. [EstratÃ©gia de Cutover](#estratÃ©gia-de-cutover)
3. [PrÃ©-requisitos e Checklist PrÃ©-ProduÃ§Ã£o](#prÃ©-requisitos-e-checklist-prÃ©-produÃ§Ã£o)
4. [Plano de Rollback](#plano-de-rollback)
5. [Janela de ManutenÃ§Ã£o](#janela-de-manutenÃ§Ã£o)
6. [Processo de HomologaÃ§Ã£o](#processo-de-homologaÃ§Ã£o)
7. [Checklist PÃ³s-ProduÃ§Ã£o](#checklist-pÃ³s-produÃ§Ã£o)
8. [ComunicaÃ§Ã£o e Stakeholders](#comunicaÃ§Ã£o-e-stakeholders)
9. [ContingÃªncias](#contingÃªncias)

---

## VisÃ£o Geral

### Objetivo da TransiÃ§Ã£o

Substituir o **sistema legado de integraÃ§Ã£o eSocial** (baseado em batch jobs + FTP) pelo **novo Pipeline ETL event-driven** utilizando Kafka, garantindo:

- âœ… **Zero perda de dados** durante a transiÃ§Ã£o
- âœ… **Downtime mÃ­nimo** (janela de 4 horas)
- âœ… **Rollback seguro** em caso de problemas crÃ­ticos
- âœ… **ValidaÃ§Ã£o completa** antes de desativar sistema legado

### Sistemas Envolvidos

| Sistema | Papel | Status Atual | Status Futuro |
|---------|-------|--------------|---------------|
| **Sistema RH Legado** | Origem dos dados | âœ… Ativo | âœ… Permanece (PostgreSQL) |
| **Batch ETL Legado** | Processamento atual | âœ… Ativo | âŒ Desativado (apÃ³s transiÃ§Ã£o) |
| **FTP eSocial** | Envio arquivos XML | âœ… Ativo | âŒ SubstituÃ­do por API |
| **Pipeline Kafka (Novo)** | Streaming event-driven | ğŸŸ¡ HomologaÃ§Ã£o | âœ… ProduÃ§Ã£o |
| **Portal eSocial** | Destino final | âœ… Ativo | âœ… Permanece |

### Riscos Identificados

| Risco | Probabilidade | Impacto | MitigaÃ§Ã£o |
|-------|---------------|---------|-----------|
| **Perda de dados na transiÃ§Ã£o** | Baixa | CRÃTICO | ExecuÃ§Ã£o paralela (7 dias) + validaÃ§Ã£o |
| **Incompatibilidade com eSocial** | MÃ©dia | ALTO | HomologaÃ§Ã£o em ambiente gov.br |
| **Performance insuficiente** | Baixa | ALTO | Testes de carga (10k evt/s) |
| **Rollback demorado** | MÃ©dia | ALTO | Plano de rollback testado |
| **Falha humana** | MÃ©dia | MÃ‰DIO | Checklists + dupla validaÃ§Ã£o |

---

## EstratÃ©gia de Cutover

### OpÃ§Ãµes Avaliadas

#### OpÃ§Ã£o 1: Big Bang (Descartada)

**DescriÃ§Ã£o:** Desligar sistema legado e ligar novo sistema de uma vez.

**Vantagens:**
- âœ… TransiÃ§Ã£o rÃ¡pida (1 dia)
- âœ… Menos complexo

**Desvantagens:**
- âŒ Alto risco de falha
- âŒ Rollback difÃ­cil
- âŒ Zero margem de erro

**DecisÃ£o:** âŒ **Descartada** (risco muito alto)

---

#### OpÃ§Ã£o 2: Phased Rollout por RegiÃ£o (Considerada)

**DescriÃ§Ã£o:** Migrar por filiais/regiÃµes progressivamente.

**Vantagens:**
- âœ… Risco distribuÃ­do
- âœ… Aprendizado incremental

**Desvantagens:**
- âŒ Complexidade operacional (2 sistemas paralelos por meses)
- âŒ Custo elevado de manutenÃ§Ã£o dual

**DecisÃ£o:** ğŸŸ¡ **Reserva** (se OpÃ§Ã£o 3 falhar)

---

#### OpÃ§Ã£o 3: Parallel Run com Cutover Planejado (ESCOLHIDA) âœ…

**DescriÃ§Ã£o:** Executar ambos os sistemas em paralelo por 7 dias, validar resultados e fazer cutover planejado.

**Vantagens:**
- âœ… ValidaÃ§Ã£o real de dados (comparaÃ§Ã£o lado-a-lado)
- âœ… Rollback trivial (apenas desligar novo sistema)
- âœ… ConfianÃ§a alta antes do cutover
- âœ… Zero perda de dados

**Desvantagens:**
- âš ï¸ Requer 7 dias de execuÃ§Ã£o dual
- âš ï¸ Custo computacional temporÃ¡rio (2x)

**DecisÃ£o:** âœ… **ESCOLHIDA**

---

### Fases da TransiÃ§Ã£o (Parallel Run)

```

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Fase 1     â”‚   Fase 2     â”‚   Fase 3     â”‚   Fase 4     â”‚
â”‚  PreparaÃ§Ã£o  â”‚ Parallel Run â”‚   Cutover    â”‚ PÃ³s-ProduÃ§Ã£o â”‚
â”‚   (3 dias)   â”‚   (7 dias)   â”‚  (4 horas)   â”‚   (7 dias)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
D-10 a D-7      D-7 a D-0        D-Day       D+1 a D+7

Fase 1: PreparaÃ§Ã£o

- Deploy em produÃ§Ã£o (modo shadowing)
- ConfiguraÃ§Ãµes finais
- Treinamento equipe

Fase 2: Parallel Run (ValidaÃ§Ã£o)

- Legado: 100% trÃ¡fego (ativo)
- Novo: 100% trÃ¡fego (shadowing - nÃ£o envia para eSocial)
- Comparar resultados diariamente

Fase 3: Cutover

- Janela de manutenÃ§Ã£o (SÃ¡bado 02:00 - 06:00)
- Desligar batch legado
- Ativar envio real do novo sistema
- ValidaÃ§Ã£o intensiva

Fase 4: PÃ³s-ProduÃ§Ã£o

- Monitoramento 24x7
- Suporte on-call
- Manter legado desligado (standby para rollback)

```

---

## PrÃ©-requisitos e Checklist PrÃ©-ProduÃ§Ã£o

### PrÃ©-requisitos ObrigatÃ³rios

#### Infraestrutura

- [ ] **Cluster Kafka Production-ready**
  - 3 brokers (mÃ­nimo)
  - Replication Factor = 3
  - Min In-Sync Replicas = 2
  - Disk: 1TB por broker (SSD)
  - CPU: 8 cores por broker
  - RAM: 32GB por broker

- [ ] **PostgreSQL Production-ready**
  - InstÃ¢ncia dedicada (nÃ£o compartilhada)
  - CPU: 16 cores
  - RAM: 64GB
  - Disk: 500GB SSD (NVMe)
  - ReplicaÃ§Ã£o ativa (primary + standby)
  - Backup automatizado (diÃ¡rio + incremental)

- [ ] **Kubernetes Cluster** (ou Docker Swarm)
  - 5 nodes (mÃ­nimo)
  - Auto-scaling configurado
  - Health checks ativos
  - Load balancer configurado

- [ ] **Rede e SeguranÃ§a**
  - VPN para acesso portal eSocial
  - Certificado Digital A1 ou A3 instalado
  - TLS 1.3 habilitado (Kafka + PostgreSQL)
  - SASL/SCRAM autenticaÃ§Ã£o Kafka
  - Firewall rules configuradas

#### Observabilidade

- [ ] **Stack de Monitoramento**
  - Prometheus (coleta de mÃ©tricas)
  - Grafana (dashboards)
  - Alertmanager (alertas)
  - Jaeger ou Zipkin (distributed tracing)
  - ELK Stack (logs centralizados)

- [ ] **Alertas Configurados**
  - 15 alertas crÃ­ticos ativos
  - PagerDuty ou similar integrado
  - EscalaÃ§Ã£o automÃ¡tica configurada
  - Runbooks documentados

#### SeguranÃ§a e Compliance

- [ ] **LGPD / ProteÃ§Ã£o de Dados**
  - Dados sensÃ­veis criptografados (at-rest + in-transit)
  - Logs de acesso habilitados
  - RetenÃ§Ã£o de dados configurada (7 anos eSocial)
  - AnonimizaÃ§Ã£o de dados de teste

- [ ] **Auditoria**
  - Audit trail completo
  - Registros de mudanÃ§as (quem, quando, o quÃª)
  - Logs imutÃ¡veis (WORM - Write Once, Read Many)

#### Testes

- [ ] **Testes de AceitaÃ§Ã£o (UAT)**
  - 50 cenÃ¡rios de teste executados
  - 100% de sucesso nos cenÃ¡rios crÃ­ticos
  - Sign-off das Ã¡reas de negÃ³cio

- [ ] **Testes de Carga**
  - Throughput: 10.000 eventos/segundo (pico)
  - LatÃªncia P95 < 100ms
  - Stress test: 24 horas contÃ­nuas
  - Resultado: âœ… Sistema estÃ¡vel

- [ ] **Testes de IntegraÃ§Ã£o eSocial**
  - Ambiente de homologaÃ§Ã£o gov.br
  - 100 eventos de teste enviados
  - ValidaÃ§Ã£o de retorno (protocolos)
  - Certificado digital validado

#### DocumentaÃ§Ã£o

- [ ] **DocumentaÃ§Ã£o Completa**
  - Manual de OperaÃ§Ã£o (OPERATIONS_MANUAL.md)
  - Manual do Desenvolvedor (DEVELOPER_GUIDE.md)
  - Runbooks de incidentes
  - ADRs (7 decisÃµes arquiteturais)
  - Diagramas C4 Model atualizados

- [ ] **Treinamento**
  - Equipe de operaÃ§Ãµes treinada (8 horas)
  - Equipe de suporte treinada (4 horas)
  - SimulaÃ§Ã£o de incidentes realizada

---

### Checklist PrÃ©-ProduÃ§Ã£o (D-7)

#### Semana Antes do Go-Live

| # | Atividade | ResponsÃ¡vel | Prazo | Status |
|---|-----------|-------------|-------|--------|
| 1 | Deploy em produÃ§Ã£o (modo shadowing) | DevOps | D-7 | â¬œ |
| 2 | Configurar variÃ¡veis de ambiente | DevOps | D-7 | â¬œ |
| 3 | Validar conectividade com eSocial | Infra | D-7 | â¬œ |
| 4 | Configurar certificado digital | SeguranÃ§a | D-7 | â¬œ |
| 5 | Habilitar TLS/SASL no Kafka | SeguranÃ§a | D-6 | â¬œ |
| 6 | Ativar monitoramento e alertas | DevOps | D-6 | â¬œ |
| 7 | Backup completo do PostgreSQL | DBA | D-6 | â¬œ |
| 8 | Executar smoke tests | QA | D-5 | â¬œ |
| 9 | Iniciar Parallel Run | DevOps | D-7 | â¬œ |
| 10 | Validar resultados Parallel Run (diÃ¡rio) | Tech Lead | D-7 a D-1 | â¬œ |
| 11 | ReuniÃ£o Go/No-Go | Todos | D-1 | â¬œ |
| 12 | Comunicar stakeholders | PM | D-1 | â¬œ |

---

#### Checklist 1 Dia Antes (D-1)

| # | Atividade | ResponsÃ¡vel | Status |
|---|-----------|-------------|--------|
| 1 | Validar resultados finais Parallel Run | Tech Lead | â¬œ |
| 2 | Comparar 100% dos eventos (legado vs novo) | QA | â¬œ |
| 3 | Confirmar taxa de sucesso > 99.9% | QA | â¬œ |
| 4 | Executar health checks completos | DevOps | â¬œ |
| 5 | Verificar espaÃ§o em disco (> 50% livre) | Infra | â¬œ |
| 6 | Confirmar equipe on-call disponÃ­vel | PM | â¬œ |
| 7 | Testar plano de rollback (simulaÃ§Ã£o) | DevOps | â¬œ |
| 8 | Freeze de mudanÃ§as em produÃ§Ã£o | Change Manager | â¬œ |
| 9 | Comunicar usuÃ¡rios finais (RH) | PM | â¬œ |
| 10 | ReuniÃ£o Final Go/No-Go | ComitÃª | â¬œ |

**CritÃ©rio Go:** Todos os itens devem estar âœ… para prosseguir.

---

## Plano de Rollback

### CenÃ¡rios de Rollback

| CenÃ¡rio | Gatilho | Tempo de DecisÃ£o | AÃ§Ã£o |
|---------|---------|------------------|------|
| **Rollback Imediato** | Taxa de erro > 20% | 15 minutos | Desligar novo sistema, reativar legado |
| **Rollback Planejado** | Performance insatisfatÃ³ria | 2 horas | AnÃ¡lise + decisÃ£o + rollback |
| **Rollback Parcial** | Problema especÃ­fico | 1 hora | Desabilitar funcionalidade afetada |

---

### Procedimento de Rollback Completo

#### Passo-a-Passo (30 minutos)

**1. Declarar Rollback (5 min)**

```


# LÃ­der tÃ©cnico declara oficialmente

echo "ROLLBACK INICIADO - \$(date)" | tee /var/log/rollback.log

# Notificar equipe

curl -X POST https://slack.com/api/chat.postMessage \
-d "channel=\#alerts-esocial" \
-d "text=ğŸš¨ ROLLBACK EM ANDAMENTO"

```

**2. Desligar Novo Sistema (5 min)**

```


# Parar Kafka Consumer (para de consumir eventos)

kubectl scale deployment consumer-service --replicas=0

# Aguardar 30 segundos

sleep 30

# Parar Kafka Producer (para de publicar eventos)

kubectl scale deployment producer-service --replicas=0

# Verificar que nÃ£o hÃ¡ pods rodando

kubectl get pods | grep esocial

```

**3. Reativar Sistema Legado (10 min)**

```


# Conectar no servidor legado

ssh admin@legacy-etl-server

# Reativar cron jobs

crontab -e

# Descomentar:

# 0 2 * * * /opt/etl/batch-esocial.sh

# Iniciar manualmente (nÃ£o esperar cron)

sudo systemctl start etl-esocial.service

# Verificar logs

tail -f /var/log/etl-esocial.log

# Aguardar: "ETL process started successfully"

```

**4. Validar Sistema Legado (5 min)**

```


# Executar smoke test

/opt/etl/tests/smoke-test.sh

# Verificar Ãºltima execuÃ§Ã£o

psql -U admin -d esocial -c \
"SELECT MAX(processed_at) FROM etl_execution_log;"

# Deve ser < 5 minutos atrÃ¡s

# Verificar arquivos XML gerados

ls -lh /opt/etl/output/xml/ | tail -10

```

**5. Comunicar Rollback (5 min)**

```


# Notificar stakeholders

cat > /tmp/rollback-notification.txt << EOF
Assunto: [URGENTE] Rollback do Pipeline ETL eSocial

Equipe,

Foi necessÃ¡rio realizar rollback do novo Pipeline ETL eSocial para o sistema legado.

Motivo: [DESCREVER MOTIVO]
HorÃ¡rio: \$(date)
Sistemas afetados: Pipeline Kafka (desligado), Batch ETL (reativado)
Impacto: Processamento voltou ao normal via sistema legado
PrÃ³ximos passos: AnÃ¡lise de causa raiz e novo plano de deploy

Status: Sistema legado OPERACIONAL
Downtime total: XX minutos

Atenciosamente,
Time DevOps
EOF

# Enviar para lista de distribuiÃ§Ã£o

mail -s "[URGENTE] Rollback ETL eSocial" stakeholders@empresa.com \
< /tmp/rollback-notification.txt

```

---

### ValidaÃ§Ã£o PÃ³s-Rollback

| # | ValidaÃ§Ã£o | Comando | Resultado Esperado |
|---|-----------|---------|-------------------|
| 1 | Sistema legado rodando | `systemctl status etl-esocial` | active (running) |
| 2 | Ãšltimos eventos processados | `SELECT COUNT(*) FROM esocial_events WHERE processed_at > NOW() - INTERVAL '1 hour'` | > 0 |
| 3 | Arquivos XML gerados | `ls /opt/etl/output/xml/ \| wc -l` | > 10 |
| 4 | Novo sistema desligado | `kubectl get pods \| grep esocial` | No resources found |
| 5 | UsuÃ¡rios RH operando | Contato manual | âœ… OK |

---

### Post-Mortem ObrigatÃ³rio

ApÃ³s rollback, agendar reuniÃ£o de post-mortem em **24 horas**:

**Agenda:**
1. Linha do tempo do incidente
2. Causa raiz (5 Whys)
3. Impacto (usuÃ¡rios, dados, financeiro)
4. O que funcionou / o que falhou
5. AÃ§Ãµes corretivas (com responsÃ¡veis e prazos)
6. Novo plano de deploy (se aplicÃ¡vel)

---

## Janela de ManutenÃ§Ã£o

### Janela Planejada

**Data:** SÃ¡bado, [DATA], 02:00 - 06:00 (4 horas)  
**Fuso HorÃ¡rio:** America/Sao_Paulo (GMT-3)  
**Justificativa:** Menor volume de transaÃ§Ãµes (< 1% do volume diÃ¡rio)

### AnÃ¡lise de Impacto

| HorÃ¡rio | Volume Eventos | % Volume DiÃ¡rio | Impacto UsuÃ¡rios |
|---------|----------------|-----------------|------------------|
| 00:00 - 02:00 | 500 | 2% | Baixo (plantÃ£o) |
| **02:00 - 06:00** | **200** | **< 1%** | **MÃ­nimo** |
| 06:00 - 08:00 | 1.200 | 5% | MÃ©dio |
| 08:00 - 18:00 | 18.000 | 75% | Alto |

**DecisÃ£o:** Janela de 02:00 - 06:00 minimiza impacto.

---

### Cronograma Detalhado da Janela

| HorÃ¡rio | DuraÃ§Ã£o | Atividade | ResponsÃ¡vel | Rollback Point |
|---------|---------|-----------|-------------|----------------|
| 01:45 | 15 min | ReuniÃ£o War Room (kick-off) | Todos | - |
| 02:00 | 10 min | Freeze do sistema legado | DevOps | âœ… RP1 |
| 02:10 | 10 min | Backup final PostgreSQL | DBA | âœ… RP2 |
| 02:20 | 20 min | Desligar batch ETL legado | DevOps | âœ… RP3 |
| 02:40 | 10 min | Ativar Producer (novo sistema) | DevOps | - |
| 02:50 | 10 min | Ativar Consumer (novo sistema) | DevOps | - |
| 03:00 | 30 min | Smoke tests + validaÃ§Ã£o | QA | âœ… RP4 |
| 03:30 | 30 min | Processar backlog (eventos da janela) | DevOps | - |
| 04:00 | 60 min | Monitoramento intensivo | Todos | âœ… RP5 |
| 05:00 | 30 min | ValidaÃ§Ã£o final + dashboards | Tech Lead | - |
| 05:30 | 30 min | ReuniÃ£o Go-Live / Lessons Learned | Todos | - |
| 06:00 | - | Fim da janela de manutenÃ§Ã£o | - | - |

**Rollback Points (RP):** Momentos onde rollback pode ser iniciado com seguranÃ§a.

---

### ComunicaÃ§Ã£o Durante a Janela

**Slack Channel:** #deploy-esocial-cutover

**FrequÃªncia de Updates:**
- A cada 15 minutos (status)
- Imediato (se problema crÃ­tico)

**Template de Update:**
```

[HH:MM] Status Update

âœ… ConcluÃ­do: [Atividade]
ğŸ”„ Em andamento: [Atividade]
â³ PrÃ³ximo: [Atividade]

Problemas: Nenhum / [Descrever]
DecisÃ£o: Prosseguir / Rollback

PrÃ³ximo update: HH:MM

```

---

## Processo de HomologaÃ§Ã£o

### Ambientes de ValidaÃ§Ã£o

| Ambiente | PropÃ³sito | Dados | DuraÃ§Ã£o |
|----------|-----------|-------|---------|
| **Dev** | Desenvolvimento | SintÃ©ticos | ContÃ­nuo |
| **QA** | Testes automatizados | SintÃ©ticos | ContÃ­nuo |
| **Staging** | Testes manuais | Anonimizados (produÃ§Ã£o) | 2 semanas |
| **Pre-Prod** | Parallel Run | ProduÃ§Ã£o (shadowing) | 7 dias |
| **ProduÃ§Ã£o** | Go-Live | ProduÃ§Ã£o (real) | - |

---

### Checklist de HomologaÃ§Ã£o (Staging)

#### Funcionalidades

- [ ] **CDC (Change Data Capture)**
  - [ ] Detectar INSERT em < 5 segundos
  - [ ] Detectar UPDATE em < 5 segundos
  - [ ] Detectar DELETE em < 5 segundos
  - [ ] NÃ£o publicar eventos duplicados

- [ ] **ValidaÃ§Ãµes**
  - [ ] Camada 1 (Estrutural): 6 regras funcionando
  - [ ] Camada 2 (NegÃ³cio): 5 regras funcionando
  - [ ] Fail-fast (para no primeiro ERROR)
  - [ ] Warnings nÃ£o bloqueiam processamento

- [ ] **PersistÃªncia**
  - [ ] Dados persistidos corretamente
  - [ ] Versionamento otimista funcionando
  - [ ] Audit trail completo (trigger)
  - [ ] Offset Kafka Ãºnico (+ partition)

- [ ] **DLQ (Dead Letter Queue)**
  - [ ] Eventos invÃ¡lidos vÃ£o para DLQ
  - [ ] Reprocessamento manual funcionando
  - [ ] Max retries respeitado (3 tentativas)

- [ ] **APIs REST**
  - [ ] GET /api/v1/validation/errors (200 OK)
  - [ ] GET /api/v1/validation/dashboard (200 OK)
  - [ ] GET /api/v1/validation/dlq (200 OK)
  - [ ] POST /api/v1/validation/dlq/{id}/retry (200 OK)

#### Performance

- [ ] **Throughput**
  - [ ] Processar 1.000 eventos/segundo (mÃ­nimo)
  - [ ] Processar 10.000 eventos/segundo (pico)
  - [ ] Zero perda de dados em 24 horas

- [ ] **LatÃªncia**
  - [ ] P50 < 50ms (validaÃ§Ã£o)
  - [ ] P95 < 100ms (validaÃ§Ã£o)
  - [ ] P99 < 200ms (validaÃ§Ã£o)

- [ ] **Recursos**
  - [ ] CPU Consumer < 70% (normal)
  - [ ] RAM Consumer < 2GB
  - [ ] Heap JVM estÃ¡vel (sem memory leak)

#### Observabilidade

- [ ] **MÃ©tricas**
  - [ ] Prometheus coletando 15 mÃ©tricas
  - [ ] Dashboards Grafana carregando
  - [ ] Alertas disparando corretamente (teste)

- [ ] **Logs**
  - [ ] Logs estruturados (JSON)
  - [ ] Correlation ID em todos os logs
  - [ ] Logs centralizados (ELK ou similar)

#### SeguranÃ§a

- [ ] **AutenticaÃ§Ã£o e AutorizaÃ§Ã£o**
  - [ ] TLS 1.3 habilitado (Kafka + PostgreSQL)
  - [ ] SASL/SCRAM autenticaÃ§Ã£o Kafka
  - [ ] Certificado Digital A1 vÃ¡lido

- [ ] **Criptografia**
  - [ ] Dados em trÃ¢nsito criptografados
  - [ ] Dados em repouso criptografados
  - [ ] Senhas nÃ£o aparecem em logs

---

### Sign-off de HomologaÃ§Ã£o

| Ãrea | ResponsÃ¡vel | Data | Assinatura | ObservaÃ§Ãµes |
|------|-------------|------|------------|-------------|
| **Desenvolvimento** | MÃ¡rcio Kuroki | [DATA] | â¬œ | - |
| **QA/Testes** | [Nome] | [DATA] | â¬œ | - |
| **SeguranÃ§a** | [Nome] | [DATA] | â¬œ | - |
| **Infraestrutura** | [Nome] | [DATA] | â¬œ | - |
| **DBA** | [Nome] | [DATA] | â¬œ | - |
| **RH (NegÃ³cio)** | [Nome] | [DATA] | â¬œ | - |
| **Compliance** | [Nome] | [DATA] | â¬œ | - |

**CritÃ©rio de AprovaÃ§Ã£o:** âœ… **TODAS as Ã¡reas devem aprovar** para prosseguir para Parallel Run.

---

## Checklist PÃ³s-ProduÃ§Ã£o

### Primeiras 24 Horas (D+1)

| # | Atividade | FrequÃªncia | ResponsÃ¡vel | Status |
|---|-----------|------------|-------------|--------|
| 1 | Monitorar dashboards Grafana | ContÃ­nuo | DevOps | â¬œ |
| 2 | Verificar alertas disparados | A cada hora | DevOps | â¬œ |
| 3 | Validar eventos processados | A cada 4h | QA | â¬œ |
| 4 | Comparar com sistema legado | 2x/dia | Tech Lead | â¬œ |
| 5 | Verificar DLQ (nÃ£o deve acumular) | A cada 2h | DevOps | â¬œ |
| 6 | Revisar logs de erro | A cada 4h | DevOps | â¬œ |
| 7 | Validar integraÃ§Ã£o eSocial | 1x/dia | QA | â¬œ |
| 8 | Coletar feedback usuÃ¡rios RH | 1x/dia | PM | â¬œ |
| 9 | ReuniÃ£o de status | 3x/dia | Todos | â¬œ |

---

### Primeira Semana (D+1 a D+7)

| # | Atividade | Prazo | ResponsÃ¡vel | Status |
|---|-----------|-------|-------------|--------|
| 1 | AnÃ¡lise de mÃ©tricas (throughput, latÃªncia) | D+1 | Tech Lead | â¬œ |
| 2 | RelatÃ³rio de erros (se houver) | D+2 | QA | â¬œ |
| 3 | ValidaÃ§Ã£o de 100% dos eventos (amostragem) | D+3 | QA | â¬œ |
| 4 | Ajustes finos (se necessÃ¡rio) | D+1 a D+5 | DevOps | â¬œ |
| 5 | Treinamento adicional (se gaps identificados) | D+4 | PM | â¬œ |
| 6 | DocumentaÃ§Ã£o de liÃ§Ãµes aprendidas | D+5 | Tech Lead | â¬œ |
| 7 | Descomissionar sistema legado (standby â†’ off) | D+7 | Infra | â¬œ |
| 8 | CelebraÃ§Ã£o com equipe ğŸ‰ | D+7 | PM | â¬œ |

---

### MÃ©tricas de Sucesso (PÃ³s-ProduÃ§Ã£o)

| MÃ©trica | Baseline (Legado) | Target (Novo) | Real (D+7) | Status |
|---------|-------------------|---------------|------------|--------|
| **Throughput** | 800 evt/s | 1.200 evt/s | - | â¬œ |
| **LatÃªncia P95** | 2.500ms (batch) | < 100ms | - | â¬œ |
| **Taxa de Sucesso** | 95% | > 99% | - | â¬œ |
| **Uptime** | 98% | > 99.7% | - | â¬œ |
| **Incidentes CrÃ­ticos** | 5/mÃªs | < 1/mÃªs | - | â¬œ |
| **SatisfaÃ§Ã£o UsuÃ¡rios** | 6/10 | > 8/10 | - | â¬œ |

**CritÃ©rio de Sucesso:** 100% das mÃ©tricas atingindo ou superando o target.

---

## ComunicaÃ§Ã£o e Stakeholders

### Matriz RACI

| Atividade | ResponsÃ¡vel (R) | Aprovador (A) | Consultado (C) | Informado (I) |
|-----------|-----------------|---------------|----------------|---------------|
| **Plano de TransiÃ§Ã£o** | Tech Lead | CTO | DevOps, DBA | Todos |
| **Parallel Run** | DevOps | Tech Lead | QA | PM, RH |
| **Go/No-Go Decision** | ComitÃª | CTO | Tech Lead | Todos |
| **ExecuÃ§Ã£o Cutover** | DevOps | Tech Lead | DBA, Infra | PM, RH |
| **Rollback** | Tech Lead | CTO | DevOps | Todos |
| **Sign-off Final** | Tech Lead | CTO, RH | QA | Todos |

---

### Plano de ComunicaÃ§Ã£o

#### Antes do Go-Live (D-7 a D-1)

| PÃºblico | Mensagem | Canal | FrequÃªncia |
|---------|----------|-------|------------|
| **Time TÃ©cnico** | Status diÃ¡rio Parallel Run | Slack #esocial-deploy | DiÃ¡rio |
| **GestÃ£o RH** | PreparaÃ§Ã£o para mudanÃ§a | Email + reuniÃ£o | D-7, D-3, D-1 |
| **UsuÃ¡rios RH** | Novidades do sistema | Intranet + treinamento | D-3 |
| **C-Level** | Status executivo | Email | D-7, D-3, D-1 |

#### Durante Go-Live (D-Day)

| PÃºblico | Mensagem | Canal | FrequÃªncia |
|---------|----------|-------|------------|
| **Time TÃ©cnico** | Status em tempo real | Slack + War Room | A cada 15 min |
| **GestÃ£o RH** | Updates crÃ­ticos | Email + WhatsApp | Se problema |
| **C-Level** | Status executivo | Email | InÃ­cio, meio, fim |

#### PÃ³s Go-Live (D+1 a D+7)

| PÃºblico | Mensagem | Canal | FrequÃªncia |
|---------|----------|-------|------------|
| **Time TÃ©cnico** | Status diÃ¡rio | Slack | DiÃ¡rio |
| **GestÃ£o RH** | RelatÃ³rio de sucesso | Email + reuniÃ£o | D+1, D+3, D+7 |
| **UsuÃ¡rios RH** | Feedback e dÃºvidas | Helpdesk + FAQ | ContÃ­nuo |
| **C-Level** | RelatÃ³rio executivo final | ApresentaÃ§Ã£o | D+7 |

---

### Template de ComunicaÃ§Ã£o (Go-Live)

#### Email para UsuÃ¡rios RH (D-1)

```

Assunto: [IMPORTANTE] Nova SoluÃ§Ã£o ETL eSocial - Go-Live SÃ¡bado

Prezados(as),

Informamos que no prÃ³ximo sÃ¡bado, [DATA], das 02:00 Ã s 06:00,
realizaremos a transiÃ§Ã£o para o novo Pipeline ETL eSocial.

O QUE MUDA:
âœ… Processamento em tempo real (vs batch diÃ¡rio)
âœ… ValidaÃ§Ãµes automÃ¡ticas aprimoradas
âœ… Interface de consulta de erros
âœ… Maior confiabilidade (99.7% uptime)

O QUE NÃƒO MUDA:

- Suas atividades diÃ¡rias no sistema RH
- Processos de admissÃ£o/demissÃ£o
- Envio para o portal eSocial

JANELA DE MANUTENÃ‡ÃƒO:
Data: SÃ¡bado, [DATA]
HorÃ¡rio: 02:00 - 06:00
Impacto: Processamento pausado durante a janela

TREINAMENTO:
Link da gravaÃ§Ã£o: [URL]
Manual do usuÃ¡rio: [URL]
FAQ: [URL]

SUPORTE:
Email: suporte-esocial@empresa.com
Telefone: (11) XXXX-XXXX (24x7)
Slack: \#suporte-esocial

Atenciosamente,
Time de Tecnologia

```

---

## ContingÃªncias

### CenÃ¡rios de ContingÃªncia

#### CenÃ¡rio 1: Certificado Digital InvÃ¡lido

**Sintoma:** Erro ao enviar para eSocial (401 Unauthorized)

**Impacto:** CRÃTICO (bloqueio total)

**SoluÃ§Ã£o:**
1. Verificar validade do certificado (not after)
2. Renovar certificado (A1 ou A3)
3. Reinstalar no servidor (keystore)
4. Reiniciar serviÃ§os

**Tempo de ResoluÃ§Ã£o:** 30 minutos (se certificado disponÃ­vel)

**PrevenÃ§Ã£o:** Alertas de expiraÃ§Ã£o (30 dias de antecedÃªncia)

---

#### CenÃ¡rio 2: Cluster Kafka IndisponÃ­vel

**Sintoma:** Producer nÃ£o consegue publicar eventos

**Impacto:** ALTO (acÃºmulo de eventos)

**SoluÃ§Ã£o:**
1. Verificar saÃºde dos brokers (`kafka-broker-api-versions`)
2. Reiniciar broker problemÃ¡tico
3. Se cluster todo down: verificar Zookeeper
4. Failover para DR cluster (se disponÃ­vel)

**Tempo de ResoluÃ§Ã£o:** 15 minutos

**PrevenÃ§Ã£o:** Monitoramento contÃ­nuo + alertas

---

#### CenÃ¡rio 3: PostgreSQL com Performance Degradada

**Sintoma:** PersistÃªncia lenta (P95 > 500ms)

**Impacto:** MÃ‰DIO (consumer lag)

**SoluÃ§Ã£o:**
1. Verificar queries lentas (`pg_stat_statements`)
2. Criar Ã­ndices faltantes
3. Ajustar connection pool (HikariCP)
4. Escalar verticalmente (se necessÃ¡rio)

**Tempo de ResoluÃ§Ã£o:** 1 hora

**PrevenÃ§Ã£o:** Testes de carga + Ã­ndices bem planejados

---

#### CenÃ¡rio 4: Taxa de Erro Elevada (> 15%)

**Sintoma:** Muitos eventos na DLQ

**Impacto:** MÃ‰DIO (dados nÃ£o processados)

**SoluÃ§Ã£o:**
1. Analisar erros (`GET /api/v1/validation/errors`)
2. Identificar regra problemÃ¡tica
3. Ajustar severidade (ERROR â†’ WARNING) se aplicÃ¡vel
4. Corrigir dados na origem
5. Reprocessar DLQ

**Tempo de ResoluÃ§Ã£o:** 2 horas

**PrevenÃ§Ã£o:** ValidaÃ§Ã£o rigorosa em homologaÃ§Ã£o

---

## AprovaÃ§Ãµes Finais

### ComitÃª de AprovaÃ§Ã£o

| Papel | Nome | Data | Assinatura | DecisÃ£o |
|-------|------|------|------------|---------|
| **CTO** | [Nome] | [DATA] | â¬œ | GO / NO-GO |
| **Tech Lead** | MÃ¡rcio Kuroki | [DATA] | â¬œ | GO / NO-GO |
| **Gerente RH** | [Nome] | [DATA] | â¬œ | GO / NO-GO |
| **Gerente Infra** | [Nome] | [DATA] | â¬œ | GO / NO-GO |
| **Compliance** | [Nome] | [DATA] | â¬œ | GO / NO-GO |
| **SeguranÃ§a** | [Nome] | [DATA] | â¬œ | GO / NO-GO |

**CritÃ©rio:** Unanimidade para GO (todos devem aprovar).

---

## Anexos

### Anexo A: Scripts de ValidaÃ§Ã£o

```

\#!/bin/bash

# scripts/validate-production.sh

echo "Validando ambiente de produÃ§Ã£o..."

# 1. Health checks

echo "1. Health checks..."
curl -s http://producer-service:8081/actuator/health | jq '.status'
curl -s http://consumer-service:8082/actuator/health | jq '.status'

# 2. Kafka

echo "2. Verificando Kafka..."
kafka-broker-api-versions --bootstrap-server localhost:9092 | wc -l

# Deve retornar 3 (3 brokers)

# 3. PostgreSQL

echo "3. Verificando PostgreSQL..."
psql -U esocial_user -d esocial -c "SELECT 1"

# 4. Certificado Digital

echo "4. Verificando certificado..."
keytool -list -keystore /etc/ssl/certs/esocial-cert.jks

# 5. MÃ©tricas

echo "5. Verificando mÃ©tricas..."
curl -s http://localhost:9090/api/v1/targets | jq '.data.activeTargets[] | select(.health == "up") | .labels.job'

echo "ValidaÃ§Ã£o concluÃ­da!"

```

---

### Anexo B: Contatos de EmergÃªncia

| Papel | Nome | Celular | Email |
|-------|------|---------|-------|
| **Tech Lead** | MÃ¡rcio Kuroki | - | marciokuroki@gmail.com |

---

## Changelog

| VersÃ£o | Data | Autor | MudanÃ§as |
|--------|------|-------|----------|
| 1.0 | 2025-11-22 | MÃ¡rcio Kuroki | CriaÃ§Ã£o inicial |

---

**Ãšltima atualizaÃ§Ã£o:** 2025-11-22
**ResponsÃ¡vel:** MÃ¡rcio Kuroki GonÃ§alves