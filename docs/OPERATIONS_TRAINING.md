# Treinamento: Pipeline ETL eSocial - Equipe de OperaÃ§Ãµes

**VersÃ£o:** 1.0  
**Data:** 2025-11-22  
**DuraÃ§Ã£o:** 4 horas  
**PÃºblico-alvo:** Equipe de SustentaÃ§Ã£o e OperaÃ§Ãµes  
**Instrutor:** MÃ¡rcio Kuroki GonÃ§alves

---

## ğŸ“‹ Agenda do Treinamento

| HorÃ¡rio | DuraÃ§Ã£o | MÃ³dulo | Formato |
|---------|---------|--------|---------|
| 09:00 - 09:45 | 45 min | **MÃ³dulo 1:** VisÃ£o Geral da Arquitetura | ApresentaÃ§Ã£o |
| 09:45 - 10:30 | 45 min | **MÃ³dulo 2:** DemonstraÃ§Ã£o Hands-On | PrÃ¡tica |
| 10:30 - 10:45 | 15 min | â˜• **Coffee Break** | - |
| 10:45 - 11:30 | 45 min | **MÃ³dulo 3:** Monitoramento e Dashboards | Hands-On |
| 11:30 - 12:30 | 60 min | **MÃ³dulo 4:** Troubleshooting Simulado | PrÃ¡tica |
| 12:30 - 13:00 | 30 min | **Q&A e Feedback** | DiscussÃ£o |

---

## MÃ³dulo 1: VisÃ£o Geral da Arquitetura (45 min)

### Slide 1: TÃ­tulo

```

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                           â•‘
â•‘      PIPELINE ETL eSocial com Apache Kafka                â•‘
â•‘                                                           â•‘
â•‘           Treinamento para OperaÃ§Ãµes                      â•‘
â•‘                                                           â•‘
â•‘          MÃ¡rcio Kuroki GonÃ§alves - 2025                   â•‘
â•‘                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

```

---

### Slide 2: Por Que Este Projeto?

**Problema Atual (Sistema Legado):**
- âŒ Processamento batch (1x por dia)
- âŒ LatÃªncia alta (24 horas)
- âŒ Sem visibilidade de erros
- âŒ DifÃ­cil manutenÃ§Ã£o
- âŒ Baixa disponibilidade (98%)

**Nova SoluÃ§Ã£o (Pipeline Kafka):**
- âœ… Processamento em tempo real (< 5 segundos)
- âœ… ValidaÃ§Ãµes automÃ¡ticas (11 regras)
- âœ… Dashboard de monitoramento
- âœ… Arquitetura escalÃ¡vel
- âœ… Alta disponibilidade (99.7%+)

---

### Slide 3: Arquitetura de Alto NÃ­vel

```

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FLUXO DE DADOS                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Sistema RH                Producer              Kafka
(PostgreSQL)              Service               Cluster
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          â”‚            â”‚          â”‚         â”‚ Broker 1 â”‚
â”‚ Employeesâ”‚            â”‚   CDC    â”‚         â”‚ Broker 2 â”‚
â”‚  Table   â”‚â”€â”€â”€â”€5sâ”€â”€â”€â”€> | Polling  â”‚â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Broker 3 â”‚
â”‚          â”‚            â”‚          â”‚         â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                                                  â”‚
                                                  â–¼
PostgreSQL              Consumer              ValidaÃ§Ãµes
(Destino)               Service               Motor
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          â”‚            â”‚          â”‚         â”‚ 11 Regrasâ”‚
â”‚ Validatedâ”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Persist  â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Estrut + â”‚
â”‚   Data   â”‚            â”‚          â”‚         â”‚ NegÃ³cio  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                       â”‚
    â”‚                       â–¼
    â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                  â”‚   DLQ    â”‚
    â”‚                  â”‚ (Erros)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚          â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

---

### Slide 4: Componentes Principais

| Componente | Porta | O Que Faz | Seu Papel |
|------------|-------|-----------|-----------|
| **Producer Service** | 8081 | Captura mudanÃ§as no banco origem | Monitorar health |
| **Consumer Service** | 8082 | Valida e persiste dados | Monitorar latÃªncia e erros |
| **Kafka Cluster** | 9092-9094 | Transporta mensagens | Monitorar lag e throughput |
| **PostgreSQL** | 5432 | Armazena dados | Monitorar espaÃ§o e conexÃµes |
| **Grafana** | 3000 | Dashboards visuais | Acompanhar mÃ©tricas |
| **Prometheus** | 9090 | Coleta mÃ©tricas | Verificar targets UP |
| **Kafka UI** | 8090 | Interface Kafka | Inspecionar mensagens |

---

### Slide 5: Fluxo de Processamento Detalhado

**1. CDC (Change Data Capture) - Producer**
```

A cada 5 segundos:

1. Query no PostgreSQL (source.employees)
2. Detecta mudanÃ§as (INSERT/UPDATE/DELETE)
3. Cria evento JSON (EmployeeEventDTO)
4. Publica no tÃ³pico Kafka correto
    - employee-create
    - employee-update
    - employee-delete
```

**2. ValidaÃ§Ã£o - Consumer**
```

Ao receber evento:

1. Consume do Kafka
2. Executa 11 validaÃ§Ãµes (fail-fast)
    - 6 estruturais (CPF, PIS, datas...)
    - 5 de negÃ³cio (idade mÃ­nima, salÃ¡rio...)
3. SE vÃ¡lido â†’ persiste
4. SE invÃ¡lido â†’ envia para DLQ
```

**3. PersistÃªncia e Audit**
```

Dados vÃ¡lidos:

1. Salva em public.employees
2. Incrementa version (optimistic lock)
3. Trigger cria audit trail
4. Retorna sucesso
```

---

### Slide 6: Conceitos Importantes

**1. Event-Driven Architecture (EDA)**
- Sistema reage a eventos (mudanÃ§as) em tempo real
- Componentes desacoplados (independentes)
- Alta escalabilidade

**2. Dead Letter Queue (DLQ)**
- "Fila de erros" para eventos invÃ¡lidos
- Permite reprocessamento manual
- Evita perda de dados

**3. Optimistic Locking (VersÃ£o)**
- Cada alteraÃ§Ã£o incrementa `version`
- Previne concorrÃªncia (2 usuÃ¡rios alterando simultaneamente)
- HistÃ³rico completo em `audit.employees_history`

**4. Fail-Fast**
- Para no primeiro erro crÃ­tico
- Economiza processamento
- Feedback rÃ¡pido

---

### Slide 7: Seu Papel como Operador

**Responsabilidades DiÃ¡rias:**

âœ… **Monitoramento**
- Verificar dashboards Grafana 3x/dia
- Alertas Slack/Email (responder em 15 min)
- Health checks (Producer e Consumer UP?)

âœ… **InvestigaÃ§Ã£o**
- Consumer lag alto? Verificar causa
- Taxa de erro elevada? Analisar DLQ
- Performance degradada? Checar recursos

âœ… **AÃ§Ã£o Corretiva**
- Restart de serviÃ§os (se necessÃ¡rio)
- Reprocessamento de eventos DLQ
- EscalaÃ§Ã£o para dev (se bug no cÃ³digo)

âŒ **NÃƒO Ã‰ SUA RESPONSABILIDADE:**
- Corrigir bugs no cÃ³digo (escalar para dev)
- Alterar infraestrutura Kafka/PostgreSQL (DBA)
- Implementar novas validaÃ§Ãµes (dev)

---

### Slide 8: MÃ©tricas-Chave (Memorize!)

| MÃ©trica | Normal | Alerta | CrÃ­tico | AÃ§Ã£o |
|---------|--------|--------|---------|------|
| **Throughput** | 800-1.500/s | < 500/s | < 200/s | Investigar gargalo |
| **LatÃªncia P95** | 50-100ms | 200ms | > 500ms | Otimizar queries |
| **Taxa de Erro** | 5-10% | 15% | > 20% | Analisar DLQ |
| **Consumer Lag** | < 100 | 500 | > 1.000 | Escalar consumer |
| **DLQ Eventos** | < 50 | 200 | > 1.000 | Reprocessar urgente |
| **Uptime** | > 99.7% | < 99% | < 95% | Incidente crÃ­tico |

---

## MÃ³dulo 2: DemonstraÃ§Ã£o Hands-On (45 min)

### ExercÃ­cio 1: Acessar Ferramentas de Monitoramento

**Objetivo:** Familiarizar com as interfaces principais.

#### Passo 1: Acessar Grafana

```


# Abrir navegador

http://localhost:3000

# Login

UsuÃ¡rio: admin
Senha: admin

# Navegar

Dashboards â†’ Browse â†’ Overview Geral

```

**O que observar:**
- Total de eventos processados (Ãºltimo hora)
- Taxa de sucesso vs erro (%)
- LatÃªncia P95 (ms)
- GrÃ¡fico de throughput (linha temporal)

---

#### Passo 2: Acessar Prometheus

```


# Abrir navegador

http://localhost:9090

# Testar query

events_published_total

# Ver resultado

Graph â†’ Execute â†’ Visualizar grÃ¡fico

```

---

#### Passo 3: Acessar Kafka UI

```


# Abrir navegador

http://localhost:8090

# Explorar

Topics â†’ employee-create â†’ Messages â†’ Ver Ãºltimas mensagens
Consumer Groups â†’ esocial-consumer-group â†’ Ver lag

```

---

### ExercÃ­cio 2: Inserir Dados e Acompanhar Processamento

**Objetivo:** Ver o pipeline funcionando end-to-end.

#### Passo 1: Conectar no PostgreSQL (Origem)

```


# Terminal

docker exec -it esocial-postgres-db psql -U esocial_user -d esocial

```

#### Passo 2: Inserir Colaborador

```

-- Inserir
INSERT INTO source.employees VALUES (
'EMP999',
'12345678901',
'10011223344',
'Maria Silva Operadora',
'1990-05-20',
'2024-01-15',
NULL,
'Analista de Suporte',
'TI',
5500.00,
'ACTIVE',
NOW(),
NOW()
);

-- Verificar inserÃ§Ã£o
SELECT * FROM source.employees WHERE employee_id = 'EMP999';

```

#### Passo 3: Aguardar CDC (5 segundos)

```


# Contar atÃ© 5...

echo "Aguardando CDC processar... 5 segundos"
sleep 5

```

#### Passo 4: Verificar em Kafka UI

```

1. Abrir http://localhost:8090
2. Topics â†’ employee-create
3. Messages â†’ Procurar por "EMP999"
4. Verificar JSON do evento
```

#### Passo 5: Verificar Processamento (PostgreSQL Destino)

```

-- Verificar dados processados
SELECT source_id, full_name, version FROM public.employees
WHERE source_id = 'EMP999';

-- Resultado esperado:
-- source_id | full_name               | version
-- EMP999    | Maria Silva Operadora   | 1

-- Verificar audit trail
SELECT operation, full_name, version, changed_at
FROM audit.employees_history
WHERE source_id = 'EMP999'
ORDER BY changed_at DESC;

-- Resultado esperado:
-- operation | full_name               | version | changed_at
-- INSERT    | Maria Silva Operadora   | 1       | 2025-11-22 10:15:32

```

#### Passo 6: Verificar no Grafana

```

1. Abrir Grafana â†’ Dashboard Overview
2. Ver "Total de Eventos" incrementou (+1)
3. Ver "Taxa de Sucesso" manteve 100%
4. Ver grÃ¡fico de linha com spike no momento da inserÃ§Ã£o
```

âœ… **Sucesso!** VocÃª acompanhou um evento do inÃ­cio ao fim.

---

### ExercÃ­cio 3: Consultar Erros e DLQ

**Objetivo:** Aprender a identificar e analisar erros.

#### Passo 1: Inserir Dado InvÃ¡lido (CPF errado)

```

-- Inserir com CPF invÃ¡lido (9 dÃ­gitos ao invÃ©s de 11)
INSERT INTO source.employees VALUES (
'EMP998',
'123456789',  -- â† CPF invÃ¡lido!
'10011223344',
'JoÃ£o Erro',
'1990-01-01',
'2024-01-01',
NULL,
'Testador',
'QA',
5000.00,
'ACTIVE',
NOW(),
NOW()
);

```

#### Passo 2: Aguardar Processamento (5 segundos)

```

sleep 5

```

#### Passo 3: Verificar Erro na API REST

```


# Consultar erros de validaÃ§Ã£o

curl http://localhost:8082/api/v1/validation/errors | jq '.[] | select(.sourceId == "EMP998")'

# Resultado esperado:

# {

# "id": 1,

# "sourceId": "EMP998",

# "ruleName": "INVALID_CPF_FORMAT",

# "errorMessage": "CPF '123456789' deve ter 11 dÃ­gitos numÃ©ricos",

# "severity": "ERROR",

# "fieldName": "cpf",

# "createdAt": "2025-11-22T10:20:15"

# }

```

#### Passo 4: Verificar Evento na DLQ

```


# Consultar DLQ

curl http://localhost:8082/api/v1/validation/dlq | jq '.[] | select(.eventId | contains("EMP998"))'

# Resultado esperado:

# {

# "id": 1,

# "eventId": "evt-abc-123",

# "eventType": "CREATE",

# "errorMessage": "INVALID_CPF_FORMAT: CPF invÃ¡lido",

# "retryCount": 0,

# "maxRetries": 3,

# "status": "PENDING",

# "canRetry": true

# }

```

#### Passo 5: Reprocessar Evento (ApÃ³s CorreÃ§Ã£o)

```

-- 1. Corrigir CPF na origem
UPDATE source.employees
SET cpf = '12345678901', updated_at = NOW()
WHERE employee_id = 'EMP998';

-- 2. Aguardar CDC (5 segundos)

```

```


# 3. Reprocessar evento da DLQ

curl -X POST http://localhost:8082/api/v1/validation/dlq/1/retry

# Resultado esperado:

# {

# "success": true,

# "message": "Event reprocessed successfully"

# }

```

---

## MÃ³dulo 3: Monitoramento e Dashboards (45 min)

### ExercÃ­cio 4: Navegar Dashboards Grafana

#### Dashboard 1: Overview Geral

**LocalizaÃ§Ã£o:** Grafana â†’ Dashboards â†’ Overview Geral

**PainÃ©is Importantes:**

1. **Total de Eventos Processados (Hoje)**
   - NÃºmero grande no topo
   - ComparaÃ§Ã£o com ontem (%)
   - **Normal:** > 10.000 eventos/dia

2. **Taxa de Sucesso vs Erro**
   - GrÃ¡fico de pizza (verde vs vermelho)
   - **Normal:** > 90% verde

3. **LatÃªncia P95 (ValidaÃ§Ã£o)**
   - Gauge (velocÃ­metro)
   - **Normal:** < 100ms
   - **Alerta:** > 200ms

4. **Throughput (Eventos/minuto)**
   - GrÃ¡fico de linha temporal
   - **Normal:** 40-60 evt/min
   - **Pico:** atÃ© 100 evt/min

5. **Consumer Lag**
   - GrÃ¡fico de Ã¡rea
   - **Normal:** < 100 eventos
   - **Alerta:** > 500 eventos

---

#### Dashboard 2: ValidaÃ§Ãµes

**LocalizaÃ§Ã£o:** Grafana â†’ Dashboards â†’ Validation Dashboard

**PainÃ©is Importantes:**

1. **Erros por Regra de ValidaÃ§Ã£o**
   - GrÃ¡fico de barras horizontais
   - Top 10 regras mais violadas
   - **AÃ§Ã£o:** Se uma regra domina (> 50%), investigar dados origem

2. **Eventos na DLQ (HistÃ³rico)**
   - GrÃ¡fico de linha
   - **Normal:** Linha estÃ¡vel prÃ³xima a zero
   - **Problema:** Linha crescente (acumulando)

3. **Taxa de Reprocessamento DLQ**
   - Percentual de eventos reprocessados com sucesso
   - **Bom:** > 80%
   - **Ruim:** < 50% (dados ruins)

---

### ExercÃ­cio 5: Criar Alerta Personalizado (Prometheus)

**Objetivo:** Configurar alerta para consumer lag alto.

#### Passo 1: Editar Arquivo de Regras

```


# Editar alert-rules.yml

docker exec -it esocial-prometheus vi /etc/prometheus/alert-rules.yml

```

#### Passo 2: Adicionar Regra

```

groups:

- name: consumer_alerts
rules:
    - alert: ConsumerLagHigh
expr: kafka_consumergroup_lag{group="esocial-consumer-group"} > 1000
for: 5m
labels:
severity: critical
annotations:
summary: "Consumer lag alto detectado"
description: "Lag de {{ \$value }} eventos no grupo esocial-consumer-group"

```

#### Passo 3: Recarregar Prometheus

```


# Reload configuraÃ§Ã£o (sem restart)

curl -X POST http://localhost:9090/-/reload

```

#### Passo 4: Testar Alerta

```


# Simular lag alto (parar consumer temporariamente)

docker stop esocial-consumer-service

# Aguardar 5 minutos

# Verificar alerta disparado

# Prometheus â†’ Alerts â†’ ConsumerLagHigh (Firing)

# Reativar consumer

docker start esocial-consumer-service

```

---

## MÃ³dulo 4: Troubleshooting Simulado (60 min)

### CenÃ¡rio 1: Consumer Lag Alto

**SimulaÃ§Ã£o:**

```


# Instrutor executa (oculto da turma)

# Simular carga alta

docker exec esocial-producer-service bash -c \
"for i in {1..5000}; do echo 'Event \$i'; done"

```

**Sintomas ObservÃ¡veis:**
- Grafana: GrÃ¡fico de Consumer Lag subindo
- Kafka UI: Lag no consumer group aumentando
- Alerta: Email/Slack de "ConsumerLagHigh"

**Sua MissÃ£o:** Investigar e resolver.

---

**InvestigaÃ§Ã£o (Passo-a-Passo):**

**1. Confirmar o problema**
```


# Ver lag atual

docker exec esocial-kafka-broker-1 kafka-consumer-groups \
--bootstrap-server localhost:9092 \
--describe --group esocial-consumer-group

# SaÃ­da:

# TOPIC           PARTITION  LAG

# employee-create    0      2500  â† LAG ALTO!

# employee-create    1      2400

# employee-create    2      2600

```

**2. Verificar saÃºde do Consumer**
```


# Health check

curl http://localhost:8082/actuator/health | jq

# Logs recentes

docker logs esocial-consumer-service --tail=50 | grep ERROR

```

**3. Verificar recursos (CPU/RAM)**
```


# Stats do container

docker stats esocial-consumer-service --no-stream

# SaÃ­da:

# CPU%    MEM USAGE / LIMIT     MEM%

# 85%     1.8GiB / 2GiB         90%   â† MemÃ³ria alta!

```

**4. Identificar causa raiz**
- Consumer lento (processamento complexo)
- Pico de carga repentino
- MemÃ³ria insuficiente (GC excessivo)

**5. SoluÃ§Ãµes possÃ­veis**

**SoluÃ§Ã£o A: Aumentar Heap JVM (temporÃ¡rio)**
```


# Editar docker-compose.yml

environment:

- JAVA_OPTS=-Xmx2g -Xms1g  \# Aumentar de 1GB para 2GB


# Restart

docker-compose restart consumer-service

```

**SoluÃ§Ã£o B: Escalar Consumer (produÃ§Ã£o)**
```


# Kubernetes (se disponÃ­vel)

kubectl scale deployment consumer-service --replicas=3

# Resultado: 3 consumers processando em paralelo

```

**SoluÃ§Ã£o C: Aumentar PartiÃ§Ãµes (longo prazo)**
```


# Mais partiÃ§Ãµes = mais paralelismo

docker exec esocial-kafka-broker-1 kafka-topics \
--bootstrap-server localhost:9092 \
--alter --topic employee-create --partitions 6

```

**6. Validar resoluÃ§Ã£o**
```


# Monitorar lag diminuindo

watch -n 5 'docker exec esocial-kafka-broker-1 kafka-consumer-groups \
--bootstrap-server localhost:9092 \
--describe --group esocial-consumer-group | grep LAG'

# Lag deve voltar para < 100 em ~10 minutos

```

âœ… **Problema Resolvido!**

---

### CenÃ¡rio 2: Taxa de Erro Elevada (> 15%)

**SimulaÃ§Ã£o:**

```


# Instrutor insere 50 registros com CPF invÃ¡lido

docker exec esocial-postgres-db psql -U esocial_user -d esocial -c \
"INSERT INTO source.employees (employee_id, cpf, ...)
SELECT 'EMP' || generate_series(1, 50), '123', ...;"

```

**Sintomas ObservÃ¡veis:**
- Grafana: Taxa de Erro subiu para 25%
- API: 50+ erros em `/api/v1/validation/errors`
- DLQ: Acumulando eventos

**Sua MissÃ£o:** Identificar causa e mitigar.

---

**InvestigaÃ§Ã£o:**

**1. Consultar erros recentes**
```

curl http://localhost:8082/api/v1/validation/errors | \
jq 'group_by(.ruleName) | map({rule: ..ruleName, count: length}) | sort_by(.count) | reverse'

# SaÃ­da:

# { "rule": "INVALID_CPF_FORMAT", "count": 50 },  â† Problema identificado!

# { "rule": "MINIMUM_AGE", "count": 3 }

# ]

```

**2. Analisar padrÃ£o dos erros**
```


# Ver exemplos de erros

curl http://localhost:8082/api/v1/validation/errors | \
jq '.[] | select(.ruleName == "INVALID_CPF_FORMAT") | {sourceId, cpf: .fieldValue}' | head -5

# SaÃ­da:

# { "sourceId": "EMP1", "cpf": "123" }

# { "sourceId": "EMP2", "cpf": "123" }

# ...

# PadrÃ£o: Todos tÃªm CPF "123" (claramente invÃ¡lido)

```

**3. Identificar origem**
- Problema no sistema RH (entrada de dados)
- MigraÃ§Ã£o de dados mal feita
- Bug no sistema legado

**4. AÃ§Ãµes corretivas**

**AÃ§Ã£o A: Corrigir dados na origem**
```

-- Conectar no PostgreSQL
docker exec -it esocial-postgres-db psql -U esocial_user -d esocial

-- Corrigir CPFs invÃ¡lidos (exemplo: padronizar com zeros)
UPDATE source.employees
SET cpf = lpad(cpf, 11, '0'), updated_at = NOW()
WHERE length(cpf) < 11;

-- Resultado: 50 rows updated

```

**AÃ§Ã£o B: Reprocessar DLQ**
```


# Listar IDs na DLQ

curl http://localhost:8082/api/v1/validation/dlq | jq '.[].id'

# Reprocessar um por vez (ou script)

for id in {1..50}; do
curl -X POST http://localhost:8082/api/v1/validation/dlq/\$id/retry
echo "Reprocessado ID \$id"
sleep 1
done

```

**AÃ§Ã£o C: Comunicar Ã¡rea de RH**
```

Assunto: [URGENTE] Erro de validaÃ§Ã£o em massa - CPF

Time de RH,

Detectamos 50 registros com CPF invÃ¡lido ("123") inseridos hoje.

Causa: [Identificar com RH]
CorreÃ§Ã£o: JÃ¡ aplicada automaticamente
Impacto: Eventos reprocessados com sucesso

Por favor, revisar processo de entrada de dados para evitar recorrÃªncia.

Atenciosamente,
Time de OperaÃ§Ãµes

```

**5. Validar resoluÃ§Ã£o**
```


# Dashboard deve voltar ao normal

curl http://localhost:8082/api/v1/validation/dashboard | jq '.successRate'

# Resultado esperado: > 90%

```

âœ… **Problema Resolvido!**

---

### CenÃ¡rio 3: Kafka Broker Down

**SimulaÃ§Ã£o:**

```


# Instrutor derruba 1 broker (de 3)

docker stop esocial-kafka-broker-2

```

**Sintomas ObservÃ¡veis:**
- Kafka UI: Apenas 2 brokers ativos
- Logs Producer/Consumer: Warnings de conexÃ£o
- Prometheus: Alerta "KafkaBrokerDown"

**Sua MissÃ£o:** Diagnosticar e restaurar.

---

**InvestigaÃ§Ã£o:**

**1. Confirmar broker down**
```


# Listar brokers

docker exec esocial-kafka-broker-1 kafka-broker-api-versions \
--bootstrap-server localhost:9092

# SaÃ­da:

# esocial-kafka-broker-1:9092 (id: 1) â† OK

# esocial-kafka-broker-3:9094 (id: 3) â† OK

# Falta broker-2!

```

**2. Verificar status do container**
```

docker ps -a | grep kafka-broker-2

# SaÃ­da:

# Exited (137) 5 minutes ago  â† Container parado!

```

**3. Verificar logs do broker**
```

docker logs esocial-kafka-broker-2 --tail=50

# Buscar por:

# - OutOfMemoryError

# - CorruptRecordException

# - Disk full

```

**4. Restart do broker**
```


# Tentar restart simples

docker start esocial-kafka-broker-2

# Aguardar inicializaÃ§Ã£o (~30 segundos)

sleep 30

# Verificar logs

docker logs -f esocial-kafka-broker-2 | grep "started"

# Aguardar: "Kafka Server started"

```

**5. Validar recuperaÃ§Ã£o**
```


# Verificar 3 brokers ativos

docker exec esocial-kafka-broker-1 kafka-broker-api-versions \
--bootstrap-server localhost:9092 | wc -l

# Resultado esperado: 3

# Verificar replicaÃ§Ã£o OK

docker exec esocial-kafka-broker-1 kafka-topics \
--bootstrap-server localhost:9092 \
--describe --topic employee-create

# Verificar: ISR (In-Sync Replicas) = 3

```

**6. Se restart falhar**
```


# Problema grave (disco cheio, corrupÃ§Ã£o)

# ESCALAR PARA DBA/INFRA!

# TemporÃ¡rio: Sistema continua funcionando com 2 brokers

# (replication factor = 3, min.insync.replicas = 2)

```

âœ… **Broker restaurado! Sistema estÃ¡vel.**

---

## MÃ³dulo 5: Q&A e Feedback (30 min)

### Perguntas Frequentes (FAQ)

**Q1: O que faÃ§o se TODOS os brokers Kafka caÃ­rem?**

**R:** PÃ¢nico controlado! ğŸ˜…
1. Sistema para de funcionar (Producer nÃ£o publica, Consumer nÃ£o consome)
2. Verificar causa (Zookeeper down? Rede? Disco?)
3. ESCALAR IMEDIATAMENTE para Infra/DBA
4. Enquanto isso: Dados ficam acumulados no PostgreSQL origem (CDC reprocesa)
5. ApÃ³s recovery: Sistema volta sozinho (eventos nÃ£o foram perdidos)

---

**Q2: Posso apagar eventos da DLQ?**

**R:** âš ï¸ Com cuidado!
- **SIM** se evento foi reprocessado com sucesso (status = REPROCESSED)
- **SIM** se evento Ã© lixo confirmado (ex: teste)
- **NÃƒO** se ainda pode ser corrigido (status = PENDING/FAILED)
- **NUNCA** sem approval do negÃ³cio (RH)

```

-- Apagar apenas reprocessados
DELETE FROM dlq_events WHERE status = 'REPROCESSED' AND updated_at < NOW() - INTERVAL '7 days';

```

---

**Q3: Quanto tempo posso deixar o Consumer parado?**

**R:**
- **Kafka:** RetÃ©m mensagens por 7 dias (configurado)
- **Risco:** Consumer lag cresce (~5.000 evt/hora)
- **MÃ¡ximo seguro:** 4 horas (lag recuperÃ¡vel em 1 hora)
- **CrÃ­tico:** > 12 horas (lag muito alto, pode demorar dias para recuperar)

---

**Q4: Como sei se preciso escalar o Consumer?**

**R:** Indicadores:
- âœ… **Escalar** se:
  - Consumer lag > 1.000 persistente (> 1 hora)
  - CPU > 80% contÃ­nuo
  - LatÃªncia P95 > 200ms
  
- âŒ **NÃƒO escalar** se:
  - Lag temporÃ¡rio (pico de carga)
  - Recursos OK (CPU < 50%)

---

### FormulÃ¡rio de Feedback

```


# Treinamento Pipeline ETL eSocial - Feedback

**Data:** 2025-11-22
**Seu Nome:** _______________________
**Cargo:** _______________________

## AvaliaÃ§Ã£o do ConteÃºdo

| MÃ³dulo | Clareza (1-5) | Utilidade (1-5) | ComentÃ¡rios |
| :-- | :-- | :-- | :-- |
| Arquitetura | â¬œ1 â¬œ2 â¬œ3 â¬œ4 â¬œ5 | â¬œ1 â¬œ2 â¬œ3 â¬œ4 â¬œ5 |  |
| Hands-On | â¬œ1 â¬œ2 â¬œ3 â¬œ4 â¬œ5 | â¬œ1 â¬œ2 â¬œ3 â¬œ4 â¬œ5 |  |
| Monitoramento | â¬œ1 â¬œ2 â¬œ3 â¬œ4 â¬œ5 | â¬œ1 â¬œ2 â¬œ3 â¬œ4 â¬œ5 |  |
| Troubleshooting | â¬œ1 â¬œ2 â¬œ3 â¬œ4 â¬œ5 | â¬œ1 â¬œ2 â¬œ3 â¬œ4 â¬œ5 |  |

## QuestÃµes

**1. VocÃª se sente preparado para operar o sistema?**

- â¬œ Sim, totalmente
- â¬œ Sim, mas preciso de mais prÃ¡tica
- â¬œ Parcialmente
- â¬œ NÃ£o

**2. Qual mÃ³dulo foi mais Ãºtil?**

- â¬œ Arquitetura
- â¬œ Hands-On
- â¬œ Monitoramento
- â¬œ Troubleshooting

**3. O que faltou no treinamento?**

________________________________________________

**4. SugestÃµes de melhoria:**

________________________________________________

**5. DÃºvidas que ficaram:**

________________________________________________

**Obrigado pelo feedback!**

```

---

## Anexo A: Comandos Essenciais (Cheat Sheet)

```


# ====================================

# HEALTH CHECKS

# ====================================

# Producer

curl http://localhost:8081/actuator/health | jq

# Consumer

curl http://localhost:8082/actuator/health | jq

# Prometheus

curl http://localhost:9090/-/healthy

# Grafana

curl http://localhost:3000/api/health

# ====================================

# MONITORAMENTO

# ====================================

# Ver mÃ©tricas Producer

curl http://localhost:8081/actuator/prometheus | grep events_published

# Ver mÃ©tricas Consumer

curl http://localhost:8082/actuator/prometheus | grep events_consumed

# Ver consumer lag

docker exec esocial-kafka-broker-1 kafka-consumer-groups \
--bootstrap-server localhost:9092 \
--describe --group esocial-consumer-group

# ====================================

# VALIDAÃ‡Ã•ES E DLQ

# ====================================

# Listar erros

curl http://localhost:8082/api/v1/validation/errors | jq

# Dashboard de validaÃ§Ã£o

curl http://localhost:8082/api/v1/validation/dashboard | jq

# Listar eventos DLQ

curl http://localhost:8082/api/v1/validation/dlq | jq

# Reprocessar evento DLQ

curl -X POST http://localhost:8082/api/v1/validation/dlq/{id}/retry

# ====================================

# CONTAINERS

# ====================================

# Listar todos

docker-compose ps

# Restart serviÃ§o

docker-compose restart producer-service

# Ver logs

docker-compose logs -f consumer-service

# Stats (CPU/RAM)

docker stats esocial-consumer-service --no-stream

# ====================================

# KAFKA

# ====================================

# Listar tÃ³picos

docker exec esocial-kafka-broker-1 kafka-topics \
--bootstrap-server localhost:9092 --list

# Descrever tÃ³pico

docker exec esocial-kafka-broker-1 kafka-topics \
--bootstrap-server localhost:9092 \
--describe --topic employee-create

# Ver mensagens (Ãºltimas 10)

docker exec esocial-kafka-broker-1 kafka-console-consumer \
--bootstrap-server localhost:9092 \
--topic employee-create \
--from-beginning --max-messages 10

# ====================================

# POSTGRESQL

# ====================================

# Conectar

docker exec -it esocial-postgres-db psql -U esocial_user -d esocial

# Queries Ãºteis

SELECT COUNT(*) FROM public.employees;
SELECT * FROM audit.employees_history ORDER BY changed_at DESC LIMIT 10;
SELECT COUNT(*) FROM dlq_events WHERE status = 'PENDING';

```

---

**Ãšltima atualizaÃ§Ã£o:** 2025-11-22  
**VersÃ£o:** 1.0  
**Autor:** MÃ¡rcio Kuroki GonÃ§alves