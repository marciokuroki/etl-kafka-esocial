# Sistema de Alertas - Guia de Setup Completo

**Projeto:** Pipeline ETL eSocial com Apache Kafka  
**Sprint:** 3 - Card 3.6  
**VersÃ£o:** 1.0  
**Data:** 2025-11-22

---

## ğŸ“‹ Ãndice

1. [VisÃ£o Geral](#visÃ£o-geral)
2. [Arquitetura do Sistema de Alertas](#arquitetura-do-sistema-de-alertas)
3. [PrÃ©-requisitos](#prÃ©-requisitos)
4. [InstalaÃ§Ã£o e ConfiguraÃ§Ã£o](#instalaÃ§Ã£o-e-configuraÃ§Ã£o)
5. [ValidaÃ§Ã£o da InstalaÃ§Ã£o](#validaÃ§Ã£o-da-instalaÃ§Ã£o)
6. [CatÃ¡logo de Alertas](#catÃ¡logo-de-alertas)
7. [ConfiguraÃ§Ã£o de NotificaÃ§Ãµes](#configuraÃ§Ã£o-de-notificaÃ§Ãµes)
8. [Testes de Alertas](#testes-de-alertas)
9. [Troubleshooting](#troubleshooting)
10. [ManutenÃ§Ã£o](#manutenÃ§Ã£o)

---

## ğŸ¯ VisÃ£o Geral

O sistema de alertas do Pipeline eSocial monitora automaticamente a saÃºde e performance dos componentes, notificando a equipe sobre problemas crÃ­ticos antes que impactem a operaÃ§Ã£o.

### Objetivos

- âœ… **DetecÃ§Ã£o proativa** de problemas
- âœ… **NotificaÃ§Ã£o automÃ¡tica** da equipe responsÃ¡vel
- âœ… **ReduÃ§Ã£o do MTTR** (Mean Time To Recover)
- âœ… **Visibilidade** do estado do sistema 24/7

### Componentes

| Componente | FunÃ§Ã£o | Porta |
|------------|--------|-------|
| **Prometheus** | Coleta mÃ©tricas e avalia regras de alerta | 9090 |
| **Alertmanager** | Gerencia e roteia alertas | 9093 |
| **Webhook Receiver** | Recebe notificaÃ§Ãµes (dev/test) | 5001 |
| **Grafana** | VisualizaÃ§Ã£o de alertas (opcional) | 3000 |

---

## ğŸ—ï¸ Arquitetura do Sistema de Alertas
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Producer Serviceâ”‚
â”‚ Consumer Serviceâ”‚â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                     â”‚ MÃ©tricas (/actuator/prometheus)
                     â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ PROMETHEUS   â”‚
            â”‚ (Scraper)    â”‚
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚ Avalia Regras (alerts.yml)
                   â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ ALERTMANAGER â”‚
            â”‚ (Router)     â”‚    
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â–¼            â–¼            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Email  â”‚ â”‚ Slack   â”‚ â”‚ Webhook  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de Alertas

1. **Coleta**: Prometheus faz scraping das mÃ©tricas dos serviÃ§os a cada 15s
2. **AvaliaÃ§Ã£o**: Prometheus avalia regras de alerta a cada 15s
3. **Disparo**: Quando condiÃ§Ã£o Ã© atendida por tempo definido (`for`), alerta Ã© disparado
4. **Agrupamento**: Alertmanager agrupa alertas similares (evita spam)
5. **Roteamento**: Alertas sÃ£o enviados para receptores conforme severidade
6. **NotificaÃ§Ã£o**: Equipe recebe notificaÃ§Ã£o via email/Slack/webhook

---

## ğŸ“‹ PrÃ©-requisitos

### ObrigatÃ³rios

- âœ… Docker e Docker Compose instalados
- âœ… Pipeline eSocial rodando (Producer + Consumer + Kafka + PostgreSQL)
- âœ… Prometheus configurado e coletando mÃ©tricas

### Validar PrÃ©-requisitos
1. Verificar serviÃ§os bÃ¡sicos
```
docker-compose ps | grep -E "(prometheus|producer|consumer)"
```
2. Verificar mÃ©tricas disponÃ­veis
```
curl -s http://localhost:9090/api/v1/targets | jq '.data.activeTargets[] | {job: .labels.job, health: .health}'
```
3. Verificar scrape do Producer
```
curl -s http://localhost:8081/actuator/prometheus | grep events_published_total
```
4. Verificar scrape do Consumer
```
curl -s http://localhost:8082/actuator/prometheus | grep events_consumed_total
```

**Resultado esperado:** Todos os serviÃ§os `UP` e mÃ©tricas disponÃ­veis.

---

## ğŸš€ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### Passo 1: Estrutura de DiretÃ³rios
Na raiz do projeto etl-kafka-esocial
```
mkdir -p config/alertmanager
mkdir -p config/alertmanager/templates
mkdir -p scripts/webhook
mkdir -p docs/sprint3/runbooks
```

### Passo 2: Arquivos de ConfiguraÃ§Ã£o

#### 2.1 Alertmanager Config (`config/alertmanager/config.yml`)
```
# ConfiguraÃ§Ã£o do Alertmanager para Sistema eSocial ETL

global:
  resolve_timeout: 5m

# Templates customizados
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Roteamento de alertas
route:
  receiver: 'default'
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s        # Aguarda antes de enviar primeiro alerta
  group_interval: 10s    # Intervalo entre alertas do mesmo grupo
  repeat_interval: 12h   # Repete alerta se nÃ£o resolvido
  
  routes:
    # Alertas crÃ­ticos - notificaÃ§Ã£o imediata
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 5s
      repeat_interval: 4h
      continue: true
    
    # Alertas de warning - agrupados
    - match:
        severity: warning
      receiver: 'warning-alerts'
      group_wait: 30s
      repeat_interval: 12h

# Receptores de notificaÃ§Ã£o
receivers:
  # Receptor padrÃ£o - webhook para testes
  - name: 'default'
    webhook_configs:
      - url: 'http://webhook-receiver:5001/alerts'
        send_resolved: true
        max_alerts: 0

  # Receptor para alertas crÃ­ticos
  - name: 'critical-alerts'
    webhook_configs:
      - url: 'http://webhook-receiver:5001/alerts/critical'
        send_resolved: true

  # Receptor para alertas de warning
  - name: 'warning-alerts'
    webhook_configs:
      - url: 'http://webhook-receiver:5001/alerts/warning'
        send_resolved: true

# InibiÃ§Ã£o de alertas (evitar spam)
inhibit_rules:
  # Se um alerta crÃ­tico estÃ¡ ativo, nÃ£o envia warnings relacionados
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'service']
  
  # Se serviÃ§o estÃ¡ down, nÃ£o alerta sobre outras mÃ©tricas dele
  - source_match:
      alertname: 'ServiceDown'
    target_match_re:
      alertname: '(HighErrorRate|HighLatency|HighConsumerLag)'
    equal: ['service']
```
#### 2.2 Atualizar Prometheus (`config/prometheus/prometheus.yml`)

Adicionar ao arquivo existente:
```
# Alertmanager
alerting:
    alertmanagers:
        - static_configs:
        - targets:
        - alertmanager:9093
    timeout: 10s

# Regras de alerta
rule_files:
    'alerts.yml'
```

#### 2.3 Regras de Alerta

O arquivo `config/prometheus/alerts.yml` jÃ¡ foi consolidado na resposta anterior.

#### 2.4 Webhook Receiver (Opcional - para testes)

**Arquivo:** `scripts/webhook/app.py`
```
#!/usr/bin/env python3
from flask import Flask, request, jsonify
from datetime import datetime
import logging

app = Flask(name)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(name)

def format_alert(alert):
status = alert.get('status', 'unknown').upper()
labels = alert.get('labels', {})
annotations = alert.get('annotations', {})

severity = labels.get('severity', 'info')
emoji_map = {'critical': 'ğŸš¨', 'warning': 'âš ï¸', 'info': 'â„¹ï¸'}
emoji = emoji_map.get(severity, 'ğŸ“¢')

output = [
    f"\n{'='*80}",
    f"{emoji} ALERTA {status} - {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}",
    f"{'='*80}",
    f"\nğŸ“Š Nome: {labels.get('alertname', 'N/A')}",
    f"âš ï¸  Severidade: {severity.upper()}",
    f"ğŸ”§ ServiÃ§o: {labels.get('service', 'N/A')}",
    f"ğŸ·ï¸  Componente: {labels.get('component', 'N/A')}",
    f"ğŸ“ DescriÃ§Ã£o:\n{annotations.get('description', 'N/A')}",
]

if annotations.get('action'):
    output.append(f"\nğŸ’¡ AÃ§Ãµes Recomendadas:\n{annotations['action']}")

output.append(f"\nâ° InÃ­cio: {alert.get('startsAt', 'N/A')}")
output.append(f"{'='*80}\n")

return '\n'.join(output)

@app.route('/health', methods=['GET'])
def health():
return jsonify({"status": "healthy"}), 200

@app.route('/alerts', methods=['POST'])
@app.route('/alerts/path:subpath', methods=['POST'])
def receive_alerts(subpath=None):
try:
alert_data = request.json
alerts = alert_data.get('alerts', [])

    logger.info(f"[{subpath or 'default'}] Recebidos {len(alerts)} alerta(s)")
    
    for alert in alerts:
        print(format_alert(alert))
    
    return jsonify({
        "status": "received",
        "count": len(alerts),
        "timestamp": datetime.now().isoformat()
    }), 200

except Exception as e:
    logger.error(f"Erro: {str(e)}")
    return jsonify({"error": str(e)}), 500

if name == 'main':
print("="*80)
print("ğŸš€ Webhook Receiver - Pipeline eSocial")
print("="*80)
print("ğŸ“¡ Porta: 5001")
print("ğŸ”— Endpoints: POST /alerts, /alerts/critical, /alerts/warning")
print("="*80 + "\n")
app.run(host='0.0.0.0', port=5001, debug=False)
```

**Arquivo:** `scripts/webhook/Dockerfile`
```
FROM python:3.11-slim
WORKDIR /app
RUN pip install --no-cache-dir flask==3.0.0
COPY app.py .
EXPOSE 5001
CMD ["python", "app.py"]
```
### Passo 3: Atualizar Docker Compose

Adicionar ao `docker-compose.yml`:
```
volumes:

#Adicionar ao final da seÃ§Ã£o volumes
alertmanager-data:
    services:

#Adicionar apÃ³s prometheus
alertmanager:
    image: prom/alertmanager:v0.26.0
    container_name: esocial-alertmanager
    hostname: alertmanager
    networks:
        - esocial-network
    ports:
        - "9093:9093"
    command:
        - '--config.file=/etc/alertmanager/config.yml'
        - '--storage.path=/alertmanager'
        - '--web.external-url=http://localhost:9093'
    volumes:
        - ./config/alertmanager/config.yml:/etc/alertmanager/config.yml
        - alertmanager-data:/alertmanager
    healthcheck:
    test: ["CMD", "wget", "--spider", "-q", "http://localhost:9093/-/healthy"]
    interval: 10s
    timeout: 5s
    retries: 3
    restart: unless-stopped

webhook-receiver:
    build:
    context: ./scripts/webhook
    dockerfile: Dockerfile
    image: esocial-webhook:latest
    container_name: esocial-webhook-receiver
    hostname: webhook-receiver
    networks:
        - esocial-network
    ports:
        - "5001:5001"
    restart: unless-stopped
```

### Passo 4: Deploy
#### 1. Parar stack atual (se necessÃ¡rio)
```
docker-compose down
```
#### 2. Build do webhook receiver
```
docker-compose build webhook-receiver
```
#### 3. Iniciar todos os serviÃ§os
```
docker-compose up -d
```
#### 4. Aguardar containers ficarem healthy
```
sleep 30
```
#### 5. Verificar status
```
docker-compose ps
```
---

## âœ… ValidaÃ§Ã£o da InstalaÃ§Ã£o

### 1. Verificar Containers
#### Verificar se todos estÃ£o rodando
```
docker-compose ps | grep -E "(alertmanager|webhook-receiver|prometheus)"
```
Resultado esperado:
```
esocial-alertmanager Up (healthy)
esocial-webhook-receiver Up
esocial-prometheus Up
```

### 2. Verificar Health Checks
#### Alertmanager
```
curl -s http://localhost:9093/-/healthy
```
Esperado: Alertmanager is Healthy.
#### Webhook Receiver
```
curl -s http://localhost:5001/health | jq
```
Esperado: {"status": "healthy"}
#### Prometheus
```
curl -s http://localhost:9090/-/healthy
```
Esperado: Prometheus is Healthy.

### 3. Verificar IntegraÃ§Ã£o Prometheus â†” Alertmanager
#### Listar Alertmanagers conectados ao Prometheus
```
curl -s http://localhost:9090/api/v1/alertmanagers | jq '.data.activeAlertmanagers'
```
Esperado:
```
{
"url": "http://alertmanager:9093/api/v2/alerts"
}
```

### 4. Verificar Regras de Alerta Carregadas
#### Listar grupos de regras
```
curl -s http://localhost:9090/api/v1/rules | jq '.data.groups[] | {name: .name, rules: (.rules | length)}'
```
Esperado:
```
{"name": "producer_critical_alerts", "rules": 4}
{"name": "consumer_critical_alerts", "rules": 6}
{"name": "kafka_performance_alerts", "rules": 2}
{"name": "infrastructure_alerts", "rules": 3}
```

### 5. Acessar Interfaces Web

#### Abrir Alertmanager UI
```
open http://localhost:9093
```
#### Abrir Prometheus Alerts
```
open http://localhost:9090/alerts
```
#### Ver targets
```
open http://localhost:9090/targets
```

---

## ğŸ“Š CatÃ¡logo de Alertas

### Resumo por Severidade

| Severidade | Quantidade | Tempo de Resposta |
|------------|------------|-------------------|
| **Critical** | 8 | < 15 minutos |
| **Warning** | 7 | < 1 hora |
| **Info** | 1 | < 24 horas |

### Alertas CrÃ­ticos (Requerem AÃ§Ã£o Imediata)

| Alerta | Componente | CondiÃ§Ã£o | For | AÃ§Ã£o |
|--------|------------|----------|-----|------|
| **ProducerServiceDown** | Producer | Service DOWN | 1m | Reiniciar serviÃ§o |
| **ConsumerServiceDown** | Consumer | Service DOWN | 1m | Reiniciar serviÃ§o |
| **ProducerHighErrorRate** | Producer | Erro > 5% | 5m | Analisar logs |
| **ConsumerHighValidationErrorRate** | Consumer | ValidaÃ§Ã£o falha > 5% | 5m | Verificar dados |
| **DLQCritical** | Consumer DLQ | Eventos > 500 | 5m | Reprocessar DLQ |
| **KafkaBrokerDown** | Kafka | Broker DOWN | 2m | Reiniciar broker |
| **PostgreSQLDown** | Database | PostgreSQL DOWN | 1m | Reiniciar DB |

### Alertas de Warning

| Alerta | Componente | CondiÃ§Ã£o | For | AÃ§Ã£o |
|--------|------------|----------|-----|------|
| **CDCHighLatency** | Producer CDC | P95 > 10s | 5m | Otimizar queries |
| **DLQAccumulating** | Consumer DLQ | Eventos > 100 | 10m | Monitorar tendÃªncia |
| **ValidationLatencyHigh** | Consumer | P95 > 5s | 5m | Otimizar validaÃ§Ã£o |
| **NoEventsProcessed** | Pipeline | Rate = 0 | 10m | Verificar origem |
| **KafkaPublishLatencyHigh** | Kafka | P95 > 1s | 5m | Verificar Kafka |
| **HighMemoryUsage** | JVM | Heap > 85% | 5m | Aumentar memÃ³ria |
| **ProducerLowThroughput** | Producer | < 1 evt/min | 10m | Verificar origem |

### Alertas Informativos

| Alerta | Componente | CondiÃ§Ã£o | For | AÃ§Ã£o |
|--------|------------|----------|-----|------|
| **HighPayloadSize** | Kafka | P95 > 10KB | 10m | Analisar estrutura |

---

## ğŸ”” ConfiguraÃ§Ã£o de NotificaÃ§Ãµes

### Email (SMTP)

Editar `config/alertmanager/config.yml`:
```
global:
smtp_smarthost: 'smtp.gmail.com:587'
smtp_from: 'alertas-esocial@empresa.com'
smtp_auth_username: 'seu-email@gmail.com'
smtp_auth_password: 'sua-senha-app' # Criar senha de app no Gmail
smtp_require_tls: true

receivers:

name: 'critical-alerts'
email_configs:

to: 'admin@empresa.com,suporte@empresa.com'
subject: 'ğŸš¨ [CRÃTICO] {{ .GroupLabels.alertname }} - Pipeline eSocial'
html: |

<h2>ğŸš¨ Alerta CrÃ­tico</h2> <p><strong>Alerta:</strong> {{ .GroupLabels.alertname }}</p> <p><strong>ServiÃ§o:</strong> {{ .GroupLabels.service }}</p> <p><strong>DescriÃ§Ã£o:</strong> {{ .CommonAnnotations.description }}</p> <p><strong>HorÃ¡rio:</strong> {{ .StartsAt.Format "02/01/2006 15:04:05" }}</p>
```

**Recarregar configuraÃ§Ã£o:**
```
docker-compose restart alertmanager
```
### Slack

1. **Criar Incoming Webhook** no Slack:
   - Acessar: https://api.slack.com/messaging/webhooks
   - Criar app e ativar Incoming Webhooks
   - Copiar URL do webhook

2. **Configurar em `config/alertmanager/config.yml`:**

```

receivers:

- name: 'critical-alerts'
slack_configs:
    - api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
channel: '\#alerts-esocial-critical'
title: 'ğŸš¨ [CRÃTICO] {{ .GroupLabels.alertname }}'
text: |
*ServiÃ§o:* {{ .GroupLabels.service }}
*Componente:* {{ .GroupLabels.component }}
*DescriÃ§Ã£o:* {{ .CommonAnnotations.description }}
*HorÃ¡rio:* {{ .StartsAt.Format "02/01/2006 15:04:05" }}
send_resolved: true

```

3. **Recarregar:**

```

docker-compose restart alertmanager

```

### PagerDuty (Opcional)

```

receivers:

- name: 'critical-alerts'
pagerduty_configs:
    - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
description: '{{ .GroupLabels.alertname }} - {{ .CommonAnnotations.summary }}'

```

---

## ğŸ§ª Testes de Alertas

### Script de Teste Automatizado

**Criar:** `scripts/test-alerts.sh`

```

\#!/bin/bash

set -e

echo "ğŸ§ª Testando Sistema de Alertas"
echo "================================"

# Teste 1: Injetar eventos invÃ¡lidos (HighValidationErrorRate)

echo -e "\n1ï¸âƒ£ Teste: Gerar erros de validaÃ§Ã£o"
for i in {1..30}; do
docker exec esocial-postgres-db psql -U esocial_user -d esocial -c "
INSERT INTO source.employees VALUES (
'TEST_ERR_\$i',
'123',  -- CPF invÃ¡lido
NULL,
'Teste Erro \$i',
'2030-01-01',  -- Data futura (invÃ¡lida)
'2024-01-01',
NULL,
'Teste',
'TI',
500.00,  -- SalÃ¡rio muito baixo
'ACTIVE',
NOW(),
NOW()
);
" > /dev/null 2>\&1
done
echo "âœ… 30 eventos invÃ¡lidos injetados"
echo "â³ Aguarde 5 minutos e verifique: http://localhost:9090/alerts"

# Teste 2: Simular Consumer Lag

echo -e "\n2ï¸âƒ£ Teste: Simular consumer lag (pausar consumer)"
docker-compose pause consumer-service
echo "â¸ï¸  Consumer pausado por 2 minutos..."
sleep 120
docker-compose unpause consumer-service
echo "â–¶ï¸  Consumer retomado"

# Teste 3: Simular Service Down

echo -e "\n3ï¸âƒ£ Teste: Simular serviÃ§o down"
docker-compose stop producer-service
echo "ğŸ›‘ Producer parado por 90 segundos..."
sleep 90
docker-compose start producer-service
echo "ğŸš€ Producer reiniciado"

echo -e "\n================================"
echo "âœ… Testes concluÃ­dos!"
echo "ğŸ“Š Verificar alertas:"
echo "   - Prometheus: http://localhost:9090/alerts"
echo "   - Alertmanager: http://localhost:9093"
echo "   - Webhook Logs: docker logs esocial-webhook-receiver -f"

```

**Executar:**

```

chmod +x scripts/test-alerts.sh
./scripts/test-alerts.sh

```

### Teste Manual de Alerta EspecÃ­fico

#### Testar ProducerServiceDown

```


# Parar Producer

docker-compose stop producer-service

# Aguardar 1 minuto (for: 1m)

# Verificar alerta disparou

curl -s http://localhost:9090/api/v1/alerts | jq '.data.alerts[] | select(.labels.alertname=="ProducerServiceDown")'

# Reiniciar Producer

docker-compose start producer-service

# Alerta deve resolver automaticamente

```

#### Testar DLQAccumulating

```


# Inserir 150 eventos invÃ¡lidos

for i in {1..150}; do
docker exec esocial-postgres-db psql -U esocial_user -d esocial -c "
INSERT INTO source.employees (employee_id, cpf, name, birth_date, hire_date, position, department, salary, status, created_at, updated_at)
VALUES ('TEST_DLQ_\$i', '123', 'Teste DLQ', '2030-01-01', '2024-01-01', 'Teste', 'TI', 100, 'ACTIVE', NOW(), NOW());
" > /dev/null 2>\&1
done

# Aguardar processamento (5 minutos)

# Verificar DLQ via API

curl -s http://localhost:8082/api/v1/validation/dlq | jq 'length'

# Verificar alerta

curl -s http://localhost:9090/api/v1/alerts | jq '.data.alerts[] | select(.labels.alertname=="DLQAccumulating")'

```

---

## ğŸ”§ Troubleshooting

### Problema: Alertas nÃ£o estÃ£o sendo disparados

**DiagnÃ³stico:**

```


# 1. Verificar se regras foram carregadas

curl -s http://localhost:9090/api/v1/rules | jq

# 2. Verificar se mÃ©tricas estÃ£o disponÃ­veis

curl -s http://localhost:9090/api/v1/query?query=up | jq

# 3. Ver logs do Prometheus

docker logs esocial-prometheus --tail 100 | grep -i "error\|warning"

```

**SoluÃ§Ã£o:**

```


# Recarregar configuraÃ§Ã£o do Prometheus

curl -X POST http://localhost:9090/-/reload

# Ou reiniciar container

docker-compose restart prometheus

```

### Problema: Alertmanager nÃ£o recebe alertas

**DiagnÃ³stico:**

```


# Verificar conexÃ£o Prometheus â†’ Alertmanager

curl -s http://localhost:9090/api/v1/alertmanagers | jq

# Ver logs do Alertmanager

docker logs esocial-alertmanager --tail 100

```

**SoluÃ§Ã£o:**

```


# Verificar configuraÃ§Ã£o do Prometheus

cat config/prometheus/prometheus.yml | grep -A 5 "alerting:"

# Reiniciar Alertmanager

docker-compose restart alertmanager

```

### Problema: Webhook nÃ£o recebe notificaÃ§Ãµes

**DiagnÃ³stico:**

```


# Verificar logs do webhook

docker logs esocial-webhook-receiver -f

# Testar webhook manualmente

curl -X POST http://localhost:5001/alerts \
-H "Content-Type: application/json" \
-d '{"alerts": [{"status": "firing", "labels": {"alertname": "Test"}, "annotations": {"description": "Teste manual"}}]}'

```

**SoluÃ§Ã£o:**

```


# Verificar conectividade Alertmanager â†’ Webhook

docker exec esocial-alertmanager wget -O- http://webhook-receiver:5001/health

# Reiniciar webhook

docker-compose restart webhook-receiver

```

### Problema: Email nÃ£o estÃ¡ sendo enviado

**DiagnÃ³stico:**

```


# Ver logs do Alertmanager

docker logs esocial-alertmanager | grep -i "email\|smtp"

```

**SoluÃ§Ãµes Comuns:**

1. **Gmail bloqueando:** Ativar "Acesso a apps menos seguros" ou criar "Senha de app"
2. **Firewall:** Verificar se porta 587 estÃ¡ aberta
3. **Credenciais:** Validar usuÃ¡rio/senha SMTP

---

## ğŸ”§ ManutenÃ§Ã£o

### Atualizar Regras de Alerta

```


# 1. Editar arquivo

vim config/prometheus/alerts.yml

# 2. Validar sintaxe

docker run --rm -v \$(pwd)/config/prometheus:/prometheus prom/prometheus:latest promtool check rules /prometheus/alerts.yml

# 3. Recarregar sem downtime

curl -X POST http://localhost:9090/-/reload

```

### Atualizar ConfiguraÃ§Ã£o do Alertmanager

```


# 1. Editar arquivo

vim config/alertmanager/config.yml

# 2. Validar sintaxe

docker exec esocial-alertmanager amtool check-config /etc/alertmanager/config.yml

# 3. Recarregar

docker exec esocial-alertmanager kill -HUP 1

```

### Silenciar Alerta Temporariamente

```


# Via API (silenciar por 2 horas)

curl -X POST http://localhost:9093/api/v2/silences \
-H "Content-Type: application/json" \
-d '{
"matchers": [
{"name": "alertname", "value": "ProducerServiceDown", "isRegex": false}
],
"startsAt": "'$(date -u +%Y-%m-%dT%H:%M:%S.%3NZ)'",
    "endsAt": "'$(date -u -d '+2 hours' +%Y-%m-%dT%H:%M:%S.%3NZ)'",
"createdBy": "admin",
"comment": "ManutenÃ§Ã£o programada"
}'

# Via UI: http://localhost:9093/\#/silences

```

### Backup de Dados do Alertmanager

```


# Backup do volume

docker run --rm \
-v esocial_alertmanager-data:/data \
-v $(pwd)/backups:/backup \
  alpine tar czf /backup/alertmanager-backup-$(date +%Y%m%d).tar.gz /data

```

